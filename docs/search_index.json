[["index.html", "GEOG0114: Principles of Spatial Analysis Welcome Structure Moodle Module tutors and contacts for GEOG0114", " GEOG0114: Principles of Spatial Analysis Welcome Welcome to GEOG0114: Principles of Spatial Analysis, one of the core 1st term modules for this MSc programme (Social and Geographic Data Science). This module has been designed as an introduction to the core tenets and principles of spatial analysis. Here, you will learn the key concepts and spatial analytical methods, which are applicable to the domains of social science and geography. In the first three weeks, we will cover Spatial analysis for data science, Graphical representation of spatial data and Spatial autocorrelation. Afterwards, in week 4, 5 and 6, we will cover a variety of basic geostatistical techniques that require analysis of point and raster data - these include suitability mapping such as Analytical Hierarchical Process (AHP) and Ecological Niche Modelling; and then Geostatistical analysis using Kriging. In week 7, 8, 9 and 10, we will learn specialised tools for performing spatial analysis on vector data (e.g., point, line and areal structures)- these include Network Analysis, Geodemographics, as well as spatial regression models approaches such as Geographical Weighted Regression (GWR) and Spatial Lag &amp; Error models. All these tutorials will be covered in RStudio. Structure All lectures and computer practicals will be delivered in-person. All Lectures are held on Monday from 02:00pm to 03:00pm at the North West Wing (Room GO7). All computer lab seminars are delivered on Monday (after lectures) from 03:00pm to 05:00pm in the same room i.e., North West Wing (Room GO7). Technical support workshops are held on Thursday from 03:00pm to 05:00pm at the Institute of Education (IOE) (Bedford Way [20])(Room 604). IMPORTANT NOTE: Please bring your own laptops with you to the computer practicals and technical support sessions on Monday and Thursday, respectively Moodle Moodle is the central point of your learning experience for GEOG0114. Please use it on a regular basis to check for updates concerning the schedule for weekly topics, access to the practical materials and assessment. However, note that all lecture notes, videos, practical materials including reading lists and downloadable data sets will be hosted on this webpage. You can download the lecture notes and data sets for the practical lesson from the table below. Week Downloads Topics 1 [Slides]; Data Spatial analysis for data science 2 [Slides]; Data Graphical representation of spatial data 3 [Slides]; Data Spatial autcorrelation 4 [Slides]; Data Suitability mapping I 5 [Slides]; [Data] Suitability mapping II Reading Week: Assessment - Spatial Analysis Project 6 [Slides]; Data Geostatistics using Kriging 7 [Slides]; [Data] Geodemographics 8 [Slides]; [Data] Transport network analysis 9 [Slides]; Data Spatial regression models I 10 [Slides]; Data Spatial regression models II Module tutors and contacts for GEOG0114 Feel free to contact us via email for help, or book appointments for additional support if need be. We are based at UCL Department of Geography, North West Wing building. Our contact information and office details are: Name Email Room number Anwar Musah a.musah@ucl.ac.uk 115 Justin van Dijk j.t.vandijk@ucl.ac.uk 117 Shun-Chan Tsai shun-chan.tsai.14@ucl.ac.uk – "],["reading-list-for-geog0114-202223.html", "Reading List for GEOG0114 2022/23 Week 1: Spatial analysis for data science Week 2: Graphical representation of spatial data Week 3: Spatial autocorrelation Week 4: Suitability Mapping I", " Reading List for GEOG0114 2022/23 Week 1: Spatial analysis for data science Book: [Theory] Longley, P. et al (2015) Geographic Information Science &amp; Systems (4th Edition); Chapters 2: The Nature of Geographic Data Click link (Note: use of your UCL login via institutional organisation (UK Federation) is required to access the digital book) Book: [Theory] Longley, P. et al (2015) Geographic Information Science &amp; Systems (4th Edition); Chapters 3: Representing Geography Click link (Note: use of your UCL login via institutional organisation (UK Federation) is required to access the digital book) Book: [R Programming] Brunsdon, C. et al (2018) An Introduction to R for Spatial Analysis and Mapping; Chapter 3: Basics of Handling Spatial Data in R Click link (Note: Books can be borrowed from UCL’s Library) Book: [R Programming] Dorman, M. (2014) Learning R for Geospatial Analysis; Chapter 1: The R Environment Click link (Note: Digital book) Book: [R Programming] Dorman, M. (2014) Learning R for Geospatial Analysis; Chapter 3: Working with tables Click link (Note: Digital book) Paper: [R Programming] Tennekes, M. (2018). tmap: Thematic Maps in R. Journal of Statistical Software, 84(6), 1–39. https://doi.org/10.18637/jss.v084.i06 (Download) Week 2: Graphical representation of spatial data Gitbook: [R Programming] Gimond, M. (2021) Intro to GIS and spatial analysis. Chapter 8: Spatial operations and vector overlays Click link Book: [Theory] Longley, P. et al (2015) Geographic Information Science &amp; systems, Chapter 13: Spatial Analysis Click link Gitbook: [R Programming] Lovelace, R., Nowosad, J. and Muenchow, J. (2021) Geocomputation with R, Chapter 4: Spatial data operations Click link Gitbook: [R Programming] Lovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 5: Geometry operations Click link Paper: [Research] Bijnens, E. et al (2020). Residential green space and child intelligence and behavior across urban, suburban, and rural areas in Belgium: A longitudinal birth cohort study of twins. PLOS Medicine 17(8), e1003213. Click link Week 3: Spatial autocorrelation Gitbook: [R Programming] Gimond, M. (2021) Intro to GIS and spatial analysis. Chapter 13: Spatial autocorrelation Click link Book: [Theory] Longley, P. et al (2015) Geographic Information Science &amp; systems (4th Edition); Chapter 2: The Nature of Geographic Data Click link Book: [Theory] Longley, P. _*et al_ (2015) Geographic Information Science &amp; systems (4th Edition); Chapter 13: Spatial Analysis Click link Paper: [Research] Radil, S. 2016. Spatial analysis of crime. In: Huebner, B. and Bynum, T. The Handbook of Measurement Issues in Criminology and Criminal Justice, Chapter 24, pp.536-554. Click link (Note: use of your UCL login via institutional organisation (UK Federation) is required to access the digital paper) Week 4: Suitability Mapping I Paper: [Theory] R.W. Saaty (1987). The Analytical Hierarchy Process - What it is and how it is used, Mathl Modelling, Volume 9(3-5), pages 161 to 176 Click link Technical Document: [Theory] IDRISI Kilimanjaro: Guide to GIS and Image Processing. Chapter 12: Decision Support - Decision Strategy Analysis, Section on ‘A Typology of Decisions’, pages 151 to 154. Click link Paper: [Application] A.A. Ali, A. Ahmad (2019). Spatial susceptibility analysis of vector-borne diseases in KMC using geospatial technique and MCDM approach, Modeling Earth Systems and Environment, Issue 5, pages 1135–1159 Click link Paper: [Application] X. Zhao et al., (2020). Malaria Risk Map Using Spatial Multi-Criteria Decision Analysis along Yunnan Border During the Pre-elimination Period, Am J Trop Med Hyg, Volume 103 (2), pages 793–809 Click link Paper: [Application] B. Ahmed (2015). Landslide susceptibility mapping using multi-criteria evaluation techniques in Chittagong Metropolitan Area, Bangladesh. Landslides, Issue 12, pages 1077–1095 Click link "],["spatial-analysis-for-data-sciences.html", "1 Spatial analysis for data sciences 1.1 Introduction 1.2 Getting started with RStudio 1.3 Basics of managing data in RStudio 1.4 Basic visualisation of spatial data in RStudio 1.5 References 1.6 Data Sources 1.7 Practical homework", " 1 Spatial analysis for data sciences 1.1 Introduction The goal for this week’s session is to get you started with using RStudio, and being familiar with its environment. The session aims to introduce you to the basic programming etiquette, as well as building confidence for using RStudio as a GIS tool. At the end of this session, you should be able to perform some basic data managing tasks as well as generate a simple choropleth map in RStudio. 1.1.1 Learning outcomes The first task includes getting you started with RStudio by installing the needed software(s) (i.e., RStudio and R (Base)) on to your personal laptop, and becoming familiar with its environment and panels. We will begin a soft introduction on the basics of managing data in RStudio. This includes learning how to create various objects in RStudio such as vector and data frame objects. The crucial part of this session we be to know how to the set working directories as well as import your dataset in RStudio. Finally, we will learn how to perform the basic visualisation of spatial data in RStudio. Let us begin. 1.2 Getting started with RStudio What is RStudio (or R)? R, or RStudio is a statistical software programming package that allows the user to carry out different types of statistical analysis. It can also be used as a GIS software to perform various kinds of spatial analysis as you will see throughout the course of GEOG0114. In the same vein, you can use it for data managing and geo-processing (i.e., importing different types of spatial formats for manipulation beforehand for spatial analysis). There are two versions: The famous icon on the left is the version for R (Base), and the one on the right is the version for RStudio. Both software packages are the same. The only difference is that RStudio is attractive, intuitive, and more importantly, it is user-friendly than Base R. So, we will be using this version (i.e., RStudio) throughout the course. Let us talk about downloading RStudio. 1.2.1 Download and install RStudio on to your laptop RStudio is an open source software, and today its the go-to software for many researchers - its highly recommended for anyone in the domains of data science, scientific research, and technical communication. It is easy to access, and easy to download and install. In order for RStudio to work you must first install R (Base). You can follow the steps and use the table below to download the correct version for your operating system (i.e., Windows or MAC). STEPS Download the file (i.e., .exe or .pkg) for R (Base) in accordance with your operating system from the links provided in the table below. Next, install it by clicking on the downloaded file (i.e., .exe or .pkg). Now, we can download the file (i.e., .exe or .dmg) for RStudio in accordance with your operating system from the links provided in the table below. You can install it by clicking on the downloaded file (i.e., .exe or .dmg). OS User type R (Base) RStudio Desktop Windows R-4.2.1-win.exe RStudio-2022.07.1-554.exe MAC R-4.2.1.pkg RStudio-2022.07.1-554.dmg 1.2.2 Becoming familiar with the panels in RStudio You should by now have opened RStudio on your laptop. When opening RStudio for the first time, you are greeted with its interface. The window is split into three panels: 1.) R Console, 2.) Environments and 3.) Files, help &amp; Output. Panel 1: The Console lets the user type in R-codes to perform quick commands and basic calculations. Panel 2: The Environments lets the user see which datasets, spatial objects and other files are currently stored in RStudio’s memory Panel 3: Under the File tab, it lets the user access other folders stored in the computer to open datasets. Under the Help tab, it also allows the user to view the help menu for codes and commands. Finally, under the Plots tab, the user can perusal his/her generated plots (e.g., histogram, scatterplot, maps etc.). The above section is the Menu Bar. You can access other functions for saving, editing, and opening a new Script File for writing codes. Opening a new Script File will reveal a fourth panel above the Console. You can open a Script File by: Clicking on the File tab listed inside the Menu Bar. A scroll down bar will reveal itself. Here, you can scroll to the section that says New File. Under New File, click on R Script. This should open a new Script File titled “Untitled 1”. Important Notes: Throughout the course, and in all practical tutorials, you will be encouraged to use an R Script for collating and saving the codes you have written for carrying out spatial analysis. However, we will start writing codes in a script later when we reach to section 1.3 of this tutorial. For now, let us start with the absolute basics, which is interacting with the R Console and using it as a basic calculator for typing simple code. 1.2.3 Using R Console as a Calculator The R console window (i.e., Panel 1) is the place where RStudio is waiting for you to tell it what to do. It will show the code you have commanded RStudio to execute, and it will also show the results from that command. You can type the commands directly into the window for execution as well. Let us start by using the console window as a basic calculator for typing in addition (+), subtraction (-), multiplication (*), division (/), exponents (^) and performing other complex sums. Click inside the R Console window and type 19+8, and press enter key button ↵ to get your answer. Quickly perform the following maths by typing them inside the R Console window: # Perform addition 19+8 # Perform subtraction 20-89 # Perform multiplication 18*20 # Perform division 27/3 # To number to a power e.g., 2 raise to the power of 8 2^8 # Perform complex sums (5*(170-3.405)/91)+1002 Aside from basic arithmetic operations, we can use some basic mathematical functions such as the exponential and logarithms: exp() is the exponential function log() is the logarithmic function Do not worry at all about these functions as you will use them later in the weeks to come for transforming variables. Perform the following by typing them inside the R Console window: # use exp() to apply an exponential to a value exp(5) # use log() to transforrm a value on to a logarithm scale log(3) 1.2.4 Creating basic objects and assigning values to it Now that we are familiar with using the console as a calculator. Let us build from this and learn one of the most important codes in RStudio which is called the Assignment Operator. This arrow symbol &lt;- is called the Assignment Operator. It is typed by pressing the less than symbol key &lt; followed by the hyphen symbol key -. It allows the user to assign values to an Object in R. Objects are defined as stored quantities in RStudio’s environment. These objects can be assigned anything from numeric values to character string values. For instance, say we want to create a numeric object called x and assign it with a value of 3. We do this by typing x &lt;- 3. When you enter the object x in the console and press enter ↵, it will return the numeric value 3. Another example, suppose we want to create a string object called y and assign it with some text \"Hello!\". We do this typing y &lt;- \"Hello!\". When you enter y in console, it will return the text value Hello. Let us create the objects a,b, c, and d and assign them with numeric values. Perform the following by typing them inside the R Console window: # Create an object called &#39;a&#39; and assign the value 17 to it a &lt;- 17 # Type the object &#39;a&#39; in console as a command to return value 17 a # Create an object called &#39;b&#39; and assign the value 10 to it b &lt;- 10 # Type the object &#39;b&#39; in console as a command to return value 10 b # Create an object called &#39;c&#39; and assign the value 9 to it c &lt;- 9 # Type the object &#39;c&#39; in console as a command to return value 9 c # Create an object called &#39;d&#39; and assign the value 8 to it d &lt;- 8 # Type the object &#39;d&#39; in console as a command to return value 8 d Notice how the objects a, b, c and d and its value are stored in RStudio’s environment panel. We can perform the following arithmetic operations with these object values: # type the following and return an answer (a + b + c + d)/5 # type the following and return an answer (5*(a-c)/d)^2 Let us create more objects but this time we will assign character string(s) to them. Please note that when typing a string of characters as data you will need to cover them with quotation marks \"...\". For example, say we want to create a string object called y and assign it with some text \"Hello!\". We do this by typing y &lt;- \"Hello!\". Try these examples of assigning the following character text to an object: # Create an object called &#39;e&#39; and assign the character string &quot;RStudio&quot; e &lt;- &quot;RStudio&quot; # Type the object &#39;e&#39; in the console as a command to return &quot;RStudio&quot; e # Create an object called &#39;f&#39;, assign character string &quot;Hello world&quot; f &lt;- &quot;Hello world&quot; # Type the object &#39;f&#39; in the console as a command to return &quot;Hello world&quot; f # Create an object called &#39;g&#39; and assign &quot;Blade Runner is amazing&quot; g &lt;- &quot;Blade Runner is amazing&quot; # Type the object &#39;g&#39; in the console to return the result g We are now familiar with using the console and assigning values (i.e., numeric and string values) to objects. The parts covered here are the initial steps and building blocks for coding and creating datasets in RStudio. Let us progress to section 1.3. Here is where the serious stuff start. We will learn the basics of managing data and some coding etiquette - this includes creating data frames, importing &amp; exporting spreadsheets, setting up work directories, column manipulations and merging two data frames. Learning these basic tasks are key for managing data in RStudio. Point of no return: From here on out - let us open a script file and type codes there instead of the Console. We are getting serious now, we will never use the Console again. 1.3 Basics of managing data in RStudio 1.3.1 How do we enter data into RStudio? As you have already seen, RStudio is an object-oriented software package and so entering data is slightly different for the usual way of inputting information into a spreadsheet (e.g., Microsoft Excel). Here, you will need to enter the information as a Vector object before combining them into a Data Frame object. Consider this crude example of data containing the additional health information for 4 people. It contains the variable (or column) names ‘id’, ‘name’, ‘height’, ‘weight’ and ‘gender’ id name height weight gender 1 Kofi 1.65 64.2 M 2 Harry 1.77 80.3 M 3 Huijun 1.70 58.7 F 4 Fatima 1.68 75.0 F Now, when entering data to RStudio it is not like Microsoft Excel where we enter data into the cells of a spreadsheet. In RStudio, data is entered as a sequence of elements and listed inside an object called a vector. For instance, if we have three age values of 12, 57 and 26 years, and we want to enter this in RStudio, we need to use the combine function c() and combine these three elements into a vector object. Hence, the code will be c(12, 57, 26). We can assign this data by typing this code as age &lt;- c(12, 57, 26). Any time you type ‘age’ into RStudio console it will hence return these three values unless you chose to overwrite it with different information. Let us look at this more closely with the 'id' variable in the above data. Each person has an ID number from 1 to 4. We are going to list the numbers 1, 2, 3 and 4 as a sequence of elements into a vector using the combine function c() and then assign it to as a vector object calling it 'id'. # Create &#39;id&#39; vector object id &lt;- c(1, 2, 3, 4) # Type the vector object &#39;id&#39; in console to see output id Now, let us enter the information the same way for the remaining columns for ‘name’, ‘height’, ‘weight’ and ‘gender’ like we did for ‘id’: # Create &#39;name&#39; vector object name &lt;- c(&quot;Kofi&quot;, &quot;Harry&quot;, &quot;Huijun&quot;, &quot;Fatima&quot;) # Create &#39;height&#39; (in meters) vector object height &lt;- c(1.65, 1.77, 1.70, 1.68) # Create &#39;weight&#39; (in kg) vector object weight &lt;- c(64.2, 80.3, 58.7, 75.0) # Create &#39;gender&#39; vector object gender &lt;- c(&quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;) Now, that we have the vector objects ready. Let us bring them together to create a proper dataset. This new object is called a Data frame. We need to list the vectors inside the data.frame() function. # Create a dataset (data frame) dataset &lt;- data.frame(id, name, height, weight, gender) # Type the data frame object &#39;dataset&#39; in console to see output dataset # You can also see dataset in a data viewer, type View() to data: View(dataset) Important Notes: The column ‘id’ is a numeric variable with integers. The second column ‘name’ is a text variable with strings. The third &amp; fourth columns ‘height’ and ‘weight’ are examples of numeric variables with real numbers with continuous measures. The variable ‘gender’ is a text variable with strings – however, this type of variable is classed as a categorical variable as individuals were categorised as either ‘M’ and ‘F’. 1.3.2 How do we create a variable based on other existing variables in our data frame? To access a variable by its name within a data frame, you will need to first type the name of the data frame followed by a $ (dollar sign), and then typing the variable’s name of interest. For instance, suppose you just want to see the height values in the Console viewer - you just type: # to access height - you need to type &#39;dataset$height&#39; dataset$height We can use other columns or variables within our data frame to create another variable. This technique is essentially important when cleaning and managing data. From this dataset, it is possible to derive the body mass index bmi from height and weight using the formula: \\(BMI = weight/height^2\\) To generate bmi into our data frame, we would need to access the height (m) and weight (kg) columns using the $ from the data frame its stored to, and apply the above formula as a code to generate the new bmi column: # Create &#39;bmi&#39; in the data frame i.e.,&#39;dataset&#39; and calculate &#39;bmi&#39; # using the $weight and $height dataset$bmi &lt;- dataset$weight/((dataset$height)^2) # View the data frame ‘dataset’ and you will see the new bmi variable inside View(dataset) You can overwrite the height (m) column to change its units into centimeters by multiplying it to 100; equally, the weight (kg) column can be overwritten and converted from units of kilograms to grams by multiplying it to 1000. # using $height and *100 dataset$height &lt;- dataset$height*100 # using $weight and *100 dataset$weight &lt;- dataset$weight*1000 # use View() the data frame ‘dataset’ and you will see the updated variables View(dataset) 1.3.3 How do we set the working directory in our computer by connecting our folder to RStudio with the setwd() function? Now, we are getting very serious here! Important action: Before we do anything - make sure to have downloaded the data set for week 1 if you haven’t done so by clicking here. In your computer, create a new folder on your desktop page and rename the folder to “GEOG0114”, and create another folder within “GEOG0114” and rename it as “Week 1”. Make sure to unzip and transfer ALL the downloaded data directly to the Week 1 folder. Now, this part of the practicals are probably the most important section of this tutorial. It’s usually the “make” or “break” phase (i.e., you ending up loving RStudio OR you hating it and not ever wanting to pick up R again). We are going to learn how to set-up a working directory. This basically refers to us connecting the RStudio to the folder containing our dataset. It allows the user to tell RStudio to open data from a folder once it knows the path location. The path location specifies the whereabouts of the data file(s) stored within a computer. Setting your directory in RStudio beforehand makes life incredibly easier in terms of finding, importing, exporting and saving data in and out of RStudio. To illustrate what a path location is – suppose on my desktop (mac/widows) there is a folder called “GEOG0114”, and within that folder, exists another folder called “Week 1”. Finally, suppose a comma separated value (.csv) data file called “London_LSOA_FireHazards.csv” is store in this last folder i.e., Week 1. If via RStudio you want to open this CSV data file located in within the “Week 1” folder. You will need to first set the path to “Week 1” in RStudio using the setwd() function. Therefore, the path location to this folder on a Windows machine would be written as follows, \"C:/Users/accountName/Desktop/GEOG0114/Week 1\". You can access this piece of information simply by: Open the GEOG0114 folder to reveal the Week 1 folder. Open the Week 1 folder in the data files are stored. Now, click on the bar at the top which shows GEOG0114 &gt; Week 1. This should highlight and show \"C:\\Users\\accountName\\Desktop\\GEOG0114\\Week 1\" (see image below): Now, copy \"C:\\Users\\accountName\\Desktop\\GEOG0114\\Week 1\" and paste the path name into the setwd() function in your R script. Lastly, change all the back slashes \\ in the path name to forward slashes / and run the code. It should look like this: setwd(\"C:/Users/accountName/Desktop/GEOG0114/Week 1\"). For Windows, the setwd() is as follows: # set work directory in windows setwd(&quot;C:/Users/accountName/Desktop/GEOG0114/Week 1&quot;) For MAC users, its marginally different. The path location would be written as follows, \"/Users/accountName/Desktop/GEOG0114/Week 1\". You can access this piece of information simply by: Right-clicking on the folder “Week 1” (not file) in which the files are stored. Hold the “Option” ⌥ key down Click Copy \"filename\" as Pathname Paste the copied path name into the function setwd() and run the code For Mac, the setwd() is as follows: # set work directory in macs setwd(&quot;/Users/accountName/Desktop/GEOG0114/Week 1&quot;) This should set the working directory. Now, let us learn how to import a CSV data into RStudio. 1.3.4 How do we import, merge and export CSV data with RStudio? 1.3.4.1 Importing data using read.csv() As you will be working mostly with comma separated value formatted data (i.e., csv) we will therefore learn how to import and export in RStudio. There are two files that we are going to import into RStudio from Week 1’s folder: London_LSOA_Deprivation_2019.csv which contains information on seven indicators (as scores) and an IMD ranking for socioeconomic deprivation for 4,836 postcodes across London in 2019. London_LSOA_FireHazards_2019.csv which contains details about the observed and estimated expected number of fire-related accidents to have occurred in residential premises with the overall number of houses in 4,836 postcodes across London in 2019. To import a csv into RStudio, we use the read.csv() function. To demonstrate this, let us import the data for fires into an data frame object and name it as Fire_data # Import data using read.csv() function Fire_data &lt;- read.csv(file=&quot;London_LSOA_FireHazards_2019.csv&quot;, header = TRUE, sep = &quot;,&quot;) Just in case…suppose if we did NOT set the working directory earlier. We would have to go through the hassle of typing the path location in the read.csv(). For windows: Fire_data &lt;- read.csv(file=&quot;C:/Users/accountName/Desktop/GEOG0114/Week 1/London_LSOA_FireHazards_2019.csv&quot;, header = TRUE, sep = &quot;,&quot;) For Mac: Fire_data &lt;- read.csv(file=&quot;/Users/accountName/Desktop/GEOG0114/Week 1/London_LSOA_FireHazards_2019.csv&quot;, header = TRUE, sep = &quot;,&quot;) I do not recommend doing it this way. Just set the work directory with setwd() to make life easier for yourself. Important Notes: The arguments used in read.csv() function – 1.) ‘file =’ is a mandatory option where you quote the name of the file to be imported; 2.) ‘header = TRUE’ option is set to TRUE which is telling RStudio that the file that is about to be imported has column names on the first row so it should not treat as observations; and 3.) ‘sep = \",\"’ we are telling RStudio that the format of the dataset is comma separated. We have imported the fire hazards data. Now, let us import the second data for deprivation using the read.csv() function and call it Deprivation_data. The code would look something as follows: # Import data using read.csv() function Deprivation_data &lt;- read.csv(file=&quot;London_LSOA_Deprivation_2019.csv&quot;, header = TRUE, sep = &quot;,&quot;) # Show viewer the data sets View(Deprivation_data) View(Fire_data) 1.3.4.2 Joining two datasets by merger using the merge() function You will often find yourself merging two or more data frames together, especially bringing together a spatial object with a non-spatial object. We cannot stress the importance of merging objects in the correct order so that the spatial attributes are preserved. In this instance, we are just dealing with two ordinary dataset which are non-spatial - i.e., one data set contains information on Fires that occurred in geographic units in London know as Lower Super Output Areas (LSOA), while the other contains its corresponding information for socioeconomic indicators that describe the LSOA. Hence, it is possible to merge the two data frames uniquely using a common key variable like LSOA_code. This task can be done using the merge function merge(). Consequently, we want the format of the merge code to look something akin to this syntax merge(target_object, selected_object, by=”LSOA”). Merging data frames is indeed a very important technique to know especially if you need to bring together event information with no spatial dimension with actual spatial data. Alright, let’s merge the deprivation data on the fire records using the LSOA_code column, and generate a bigger data frame that contains both the fire and socioeconomic information: # Using the merge() function Full_data &lt;- merge(Fire_data, Deprivation_data, by.x = “LSOA_code”, by.y = “LSOA_code”, all.x = TRUE) # View the datasets View(Full_data) Important Notes: The arguments used in merge.csv(): Fire_data is the target data frame we want something to be merged on to. Deprivation_data is the selected data frame we are using to merge with the Fire_data. by.x = “LSOA_code” option we are specifying the name of the join column from the target data frame i.e., Fire_data. by.y = “LSOA_code” option we are specifying the name of the join column from the selected data frame i.e., Deprivation_data all.x=TRUE option we are telling RStudio to retain all rows that are originally from the target data after merging regardless of whether or not they are present in the selected data frame. So even if a row from the selected data does not find a unique link with any of the rows in target data to match too - it will still preserve the target data frame by not discarding unlinked rows. But it will discard the unmatched rows from the selected data frame. 1.3.4.3 Saving your dataset using the write.csv() function Let us save a version of this as a .csv file as a saved product named “London_Fire_and_Deprivation_2019.csv”. This can be done by using the write.csv() function. It will export the data frame object into a .csv format. # Export ‘Full_data’ object as .csv into &#39;Week 1&#39; folder write.csv(Full_data, file = &quot;London_Fire_and_Deprivation_2019.csv&quot;, row.names = FALSE) Important Notes: The arguments used in merge.csv(): Full_data is an object we are exporting. It is compulsory to specify the object data frame we want to export file = is a mandatory argument. We must give a name to the file we wish to export it as with .csv extension. row.names = this is an annoying argument! It will automatically index the dataset with unique row numbers by default if we do not specify FALSE! Since the data has its own unique identifiers (i.e., LSOA_code) we specify ‘FALSE’ to not perform this action of indexing Again, suppose if you did NOT set the work directory to your folder, you will have to type the whole path location to where you want the data to be exported which could be a hassle: For Windows: write.csv(Full_data, file = &quot;C:/Users/accountName/Desktop/GEOG0114/Week 1/London_Fire_and_Deprivation_2019.csv&quot;, row.names = FALSE) For Mac: write.csv(Full_data, file = &quot;/Users/accountName/Desktop/GEOG0114/Week 1/London_Fire_and_Deprivation_2019.csv&quot;, row.names = FALSE) Again, I do not recommend doing it this way. Just set the work directory with setwd() to make life easier for yourself and to avoid R calling you out for errors. Now that we have learned a lot of the basic things in RStudio – the stuff shown in section 1.3 will be used quite a lot in future tutorials. Let us progress to the section 1.4 where will start using RStudio as a GIS software. Here, we will create our first map to display the spatial distribution of socioeconomic deprivation using the Full_data data frame object. 1.4 Basic visualisation of spatial data in RStudio 1.4.1 Installing packages into RStudio So far, we have been using functions and commands that are by default built-in RStudio. As you will become more and more proficient in RStudio, you will come to realise that there are several functions in RStudio that are in fact not built-in by default which will require external installation. For instance, the “sf” package which is called “Simply Features” allows the user to load shapefiles (a type of Vector spatial data) into RStudio’s memory. Another important package is called “tmap”, this package gives access to various functions that allows the user to write code and emulate RStudio as a GIS software. These are examples of packages with enables mapping of spatial data. They need to be installed as they not built-in programs in RStudio. For basic mapping, We will need to install following packages: devtools, stars, tmap and sf using the install.packages() and install_version() functions, and then activate the packages to make them active using the library() function. First install devtools and stars: install.packages(&quot;devtools&quot;) devtools::install_version(&quot;stars&quot;, version = &quot;0.5.3&quot;, repos = &quot;http://cran.us.r-project.org&quot;) You will be prompted to choose an update for the stars package. Type 1 for “All” and press the enter button ↵ to continue the installation for stars. Once the binaries for devtools and stars are done, install the tmap and sf packages. install.packages(&quot;sf&quot;) install.packages(&quot;tmap&quot;) Once the installation is complete, you MUST activate the packages using the library() function. Type the following to perform this action: # Active the “sf” and “tmap” packages library(&quot;sf&quot;) library(&quot;tmap&quot;) 1.4.2 Adding shapefiles in RStudio The sf package grants the user access to a function called read_sf() to read-in shapefiles into RStudio. A shapefile typically contains the geometry of the spatial features e.g., points, line segment and boundaries of an areal feature etc. The shapefile has the extension of .shp (and are always accompanied by its other supporting files with extensions .shx, .prj, .dbf and .cpg). We have two types of shapefiles: London_LSOA_Areas.shp which contains the spatial boundaries of all LSOA postcodes in London. London_Borough_Shapefile.shp which contains the spatial boundaries for all 33 Boroughs in London. We can easily load them in RStudio as Spatial Polygon objects, type into your script: # Add shapefiles for LSOA and Boroughs london_lsoa_shapefile &lt;- read_sf(&quot;London_LSOA_areas.shp&quot; london_borough_shapefile &lt;- read_sf(&quot;London_Boroughs_Shapefile.shp&quot;) We interested in visualising the data at an LSOA-level. We would need to merge the non-spatial (aka ordinary) data i.e., Full_data, uniquely in to the Spatial Polygon object we loaded in previously i.e., london_lsoa_shapefile, using the LSOA_code column. Again, we can use the merge() function to perform this task the same way we carried out the joining of the two ordinary data frames in section 1.3.4.2. Consequently, we want the format of the merge code to look something akin to this syntax merge(target_spatial_object, selected_non_spatial_object, by=”LSOA”). # Using the merge() function Spatial_data &lt;- merge(London_lsoa_shapefile, Full_data, by.x = “LSOA_code”, by.y = “LSOA_code”, all.x = TRUE) # View the datasets View(Spatial_data) 1.4.3 Mapping with ‘tmap’ functions in RStudio The tmap is the best package for creating maps in RStudio – it’s easy to code and user friendly. Let’s finally start some mapping! Here are some basic ‘tmap’ functions to be very familiar with: tmap functions What it does… tm_shape() This allows the user to add layers to the map tm_polygon() This allows the user to direct customisations the added layer specified in tm_shape(). Note: This follows immediately after tm_shape() tm_fill() Here, we specify the variable of interest we want to map in the boundaries specified in tm_shape() tm_layout() This allows the user to make heavy customisations to the main title, legends and other cosmetics to text sizes etc. tm_compass() This allows the user to add a compass visual to the map output tm_scale_bar() This allows the user to add a scale bar to the map output 1.4.3.1 Visualising the outline(s) of study area Suppose you want to visual just the outline of London’s LSOA only: # Visual outline of London’s LSOA postcodes only tm_shape(Spatial_data) + tm_polygons() # Insert the “Spatial_data” object into the command line of # tm_shape(). No customisation has been applied here. You can customise the level of transparency for both the area and borders by adding some arguments in the tm_polygon() [i.e., alpha, and border.alpha which only take values between 0 (full transparency) to 1 (100% solid)]. For example: # Controlling transparencies for borders and areas tm_shape(Spatial_data) + tm_polygons(alpha = 0.1, border.alpha = 0.4) # Insert the “Spatial_data” object into the command line of # tm_shape(), and in tm_polygon(), make the background (alpha) 10% solid # and 40% for the borders (border.alpha) 1.4.3.2 Adding another layer on top of study area Suppose you want to add another layer to show the regions (i.e., Borough) of London for which the LSOA areas reside in, you can use another tm_shape() in the code, the coding would be as follows: # Adding another layer tm_shape(Spatial_data) + tm_polygons(alpha = 0.1, border.alpha = 0.4) + tm_shape(London_borough_shapefile) + tm_polygons(alpha = 0, border.alpha = 1, border.col = &quot;black&quot;) # The background of the added layer has been rendered to full # transparency with alpha set to 0 and borders are fully solid with # border.alpha set to 1 and border.col (colour) set to “black” to appear # pronounced. 1.4.3.3 Full visualising of data in the maps Suppose you want to visual the spatial data in those postcodes, we can use the tm_fill() function. Let us visual the IMD_Decile column which has DISCRETE (or CATEGORICAL) values classed from 1 to 10 to signify areas that are deprived (with 1 being the poorest) and those that are wealthier (with 10 being the least deprived). The coding would be as follows: # Generate map IMD_Deciles tm_shape(Spatial_data) + tm_fill(&quot;IMD_Decile&quot;, style = &quot;cat&quot;, title = &quot;Deprivation (Deciles)&quot;, palette = &quot;RdYlGn&quot;) + tm_shape(london_borough_shapefile) + tm_polygons(alpha = 0, border.alpha = 1, border.col = &quot;black&quot;) + tm_scale_bar(position = c(&quot;left&quot;, &quot;bottom&quot;)) + tm_compass(type = &quot;arrow&quot;, position = c(&quot;right&quot;, &quot;top&quot;)) + tm_layout(legend.outside = TRUE, legend.title.size = 0.8, legend.text.size = 0.7, legend.title.fontface = 2) In the tm_fill(), we insert the variable of interest we want to visual. The style argument tells RStudio how to break the values into intervals for the legend and colour scheme (there are many e.g., “fixed”, “cont”, “pretty” and “cat” etc.,). We used the “cat” in the style argument because the IMD_Decile variable is categorical. The title argument applies it to the legend only (not the plot itself!). The palette argument controls the colour scheme of the legend. In this example, we used a spectral type of colour i.e., “RdYlGn” that starts from Red (to show intense poverty) to Yellow (to signify normal) and to Green (to show intense levels of being rich). Geography 101 – when creating a map, it is always best to add the North compass and scale bar. This is done with the tm_compass() and tm_scale_bar() functions. The tm_layout() allows you to make further customisations to the map such as placing the legends outside the plot region and modifying the text size etc. You can experiment with them by checking the help menu – just type: ?tm_layout() in the console. This concludes this computer lab session. Challenge yourself with the task placed in section 1.6 1.5 References 1.5.1 Recommended reading (see reading list) Book: [Theory] Longley, P. et al (2015) Geographic Information Science &amp; Systems (4th Edition); Chapters 2: The Nature of Geographic Data Click link (Note: use of your UCL login via institutional organisation (UK Federation) is required to access the digital book) Book: [Theory] Longley, P. et al (2015) Geographic Information Science &amp; Systems (4th Edition); Chapters 3: Representing Geography Click link (Note: use of your UCL login via institutional organisation (UK Federation) is required to access the digital book) Book: [R Programming] Brunsdon, C. et al (2018) An Introduction to R for Spatial Analysis and Mapping; Chapter 3: Basics of Handling Spatial Data in R Click link (Note: Books can be borrowed from UCL’s Library) Book: [R Programming] Dorman, M. (2014) Learning R for Geospatial Analysis; Chapter 1: The R Environment Click link (Note: Digital book) Book: [R Programming] Dorman, M. (2014) Learning R for Geospatial Analysis; Chapter 3: Working with tables Click link (Note: Digital book) Paper: [R Programming] Tennekes, M. (2018). tmap: Thematic Maps in R. Journal of Statistical Software, 84(6), 1–39. https://doi.org/10.18637/jss.v084.i06 (Download) 1.6 Data Sources UK Fire statistics incident level dataset (see: “Low-level geography dataset”)[Source: UK Gov] Click Here English indices for Multiple Deprivation (IMD) for 2019 [Source: UK Gov] Click Here UK Shape files for different geographic units [Source: Office for National Statistics, Open Geography Portal] Click Here 1.7 Practical homework Exploring the burden of fire hazards and vulnerability in London An incidence rate (IR) is typically a measure of frequency used to quantify the occurrence of an event. This basic quantitative measure is often applied in social sciences (e.g., criminology for crime rates, in education on pass rates for grades), epidemiology and disaster sciences as well. The study of fire hazards and safety is an intersection of these three fields. Objectives: Use the spatial data you have built throughout the exercise to visual the distribution of incidence of fires that occurred in residential premises across postcodes in London in 2019. CLUE: The incidence rate (I) formula for calculating fires hazards across LSOAs in London is the reported number of residential premises in an LSOA that experienced a fire event (E) divided by the total number of residential premises in an LOSA (T). This is shown below expressed per 1,000 residence (per capita). The formula is: \\(I = (E/T) × 1000\\) Experiment the tmap functions and arguments "],["graphical-representation-of-spatial-data.html", "2 Graphical representation of spatial data 2.1 Introduction 2.2 Case study 2.3 Loading our data sets for case study 2.4 Data Processing 2.5 Greenspace in London 2.6 Attributions 2.7 References (see reading list) 2.8 Data Sources", " 2 Graphical representation of spatial data 2.1 Introduction The goal for this week’s session is to introduce you to the various and most common geoprocessing (i.e., spatial operations) and data managing techniques for handling geospatial vector datasets. Geoprocessing is a framework and set of tools for processing geographic and related data - having this under your sleeve will help you to efficiently analyse and represent geospatial data. 2.1.1 Learning outcomes Understanding spatial properties, relationships and how they are used within spatial operations are the building blocks to spatial data processing and analysis. This tutorial takes you through a simple approach to measuring greenspace access for schools in London, using geometric operations as the main methods for processing and analysing your data. You will construct a buffer data set around our greenspace and determine whether nearby schools intersect with this buffer. We will first visualise our data as points to see if we can identify areas of high versus low access - and then aggregate the data to the ward level for potential further use within analysis with statistical data, such as census information. 2.1.2 Getting started To enable the efficient, repeatable and reproducible functionality of our work, we will use R-Studio’s ability to create and host code as a script. Before we do anything therefore, we will need to create a new R script: File &gt; New File &gt; R Script Let’s go ahead and save our script now, so we know it’s stored in our system - and in the future, we only need to remind ourselves to complete a quick save (e.g. cmd + s (mac) / ctrl + s (windows)). 2.1.3 Datasets &amp; setting up the work directory Before you begin, if you have not done so already, please make sure to download all data by clicking here. Create a sub folder called “Week 2” within your “GEOG0114” folder stored in the desktop of your personal computer. Make sure to extract all data from the downloaded zip folder and store it into “Week 2” folder. Use your newly open R script and set the work directory to Week 2’s folder. For Windows, the code for setting the work directory will be: setwd(&quot;C:/Users/AccountName/Desktop/GEOG0114/Week 2&quot;) For MAC, the code for setting the work directory will be: setwd(&quot;/Users/AccountName/Desktop/GEOG0114/Week 2&quot;) 2.1.4 Loading and installing packages We will need to load the following packages: sf: Simple Features tmap: Thematic Mapping The above packages sf and tmap should have been installed previously in the last session. Therefore, we do not have to install them again, we only need to activate them using the library() function. There will be some new packages we need to install: tidyverse: this library is a efficient coding style in RStudio. It allows the use of a pipe function (%&gt;%), among other things, within our work and enables more efficient programming. units: this library provide functions that support the conversion of measurement units in R vectors, matrices and arrays, and among other thing, the simplification of units. # install the packages using the install.package() install.packages(&quot;tidyverse&quot;) install.packages(&quot;units&quot;) # Load the packages with library() library(&quot;tidyverse&quot;) library(&quot;units&quot;) library(&quot;sf&quot;) library(&quot;tmap&quot;) 2.2 Case study Recent research (Bijnens et al. 2020) has shown that children brought up in proximity to greenspace have a higher IQ and fewer behavioral problems, irrespective of socio-economic background. In our analysis today, we will look to understand whether there are geographical patterns to schools that have high versus low access of greenspace and where a lack of greenspace needs to be addressed in London. In this practical we will try to quantify these visual patterns we may observe and find out which schools are within 400 metres of greenspace that is larger than 50,000 square meters. We then calculate for each ward the percentage of schools that have access to a large greenspace. 2.3 Loading our data sets for case study For this analysis we have three different data sets available: schools in London, greenspace in London (split into two separate data sets), and wards (an administrative geography) in London. All three of our data sets are provided as shapefiles which will make working with the data relatively straight-forward (e.g. even for our point data, the schools, we do not need to convert them from a csv as we often find with this type of data). But we’ll need to do quite a few steps of processing to get our final data set. Let’s go ahead and load our three variables - we will use the sf library st_read() command to load our data sets into variables for use within our code: # load london schools shapefile london_schools &lt;- st_read(&#39;school_data_london_2016.shp&#39;) # load london wards shapefile london_wards &lt;- st_read(&#39;london_wards.shp&#39;) # load the two london greenspace shapefiles TL_greenspace &lt;- st_read(&#39;TL_GreenspaceSite.shp&#39;) TQ_greenspace &lt;- st_read(&#39;TQ_GreenspaceSite.shp&#39;) To see what each variable looks like, you can type in plot(name_of_variable) into the R console. This is a quick command to understand both the spatial coverage and attributes of your data - as it will display the data by each of its attribute fields as a plot. 2.4 Data Processing Now we have our data loaded as variables, we’re ready to start processing! In spatial data processing, the question always is: where do I start first? And the easiest answer to that is: make sure all of your data is in the same Projected (or Coordinate) Reference System as each other. Checking - and changing projections - should always be the first step of any workflow as this will ensure you do not carry through any potential mistakes or errors that using the wrong system can cause. 2.4.1 Reprojecting When you loaded your data sets in the above step, you may have notice that in the console additional information about the data set is printed - this includes the metadata on the data set’s Coordinate Reference System! As a result, it is quite easy to simply scroll the terminal to check the CRS for each data set - which as you’ll see, all the data sets bar the school are using EPSG 27700, which is the code for British National Grid, whereas our schools data set shows 3857, the code for Web Mercator. That means we need to start with our london_schools variable - as we know that this is the only data set currently in the wrong projection instead of using British National Grid. To reproject our data set, we can use a function within the sf library, known as st_transform(). It is very simple to use - you only need to provide the function with the data set and the code for the new CRS you wish to use with the data. For now, we will simply store the result of this transformation as a new variable - but you could in the future, rewrite this code to use pipes to pipe this transformation when loading the data set. # reproject london schools from Web Mercator to BNG london_schools_prj &lt;- st_transform(london_schools, 27700) We can now double-check our new variable is in the correct CRS by typing the following into the console and checking the result: # check CRS st_crs(london_schools_prj) As you can see from the output above, our data set has been reprojected into EPSG 27700 or British National Grid! The next step to process our london_schools_prj data set is to reduce the schools to only our chosen London extent. As you may have seen from the map above, our schools cover an area larger than our usual London extent. We can even make a quick map of this to check this properly: # inspect tm_shape(london_wards) + tm_polygons() + tm_shape(london_schools_prj) + tm_dots() As we can see, we indeed have schools outside of our London wards - as a result, we want to remove those schools outside of this boundary. We will do this by first dissolving our ward file to create a more simplified shapefile for use as a “cookie-cutter”. 2.4.2 Dissolving To dissolve a polygon shapefile using R code, we will use the summarise() function that comes from the dplyr library (part of the tidyverse) and summarise our London wards data set by summing its total area (supplied in the HECTARES attribute field/column) across all records. This will reduce our data frame to a single row, which will only contain one attribute - our total area of London, which we can then map/use as our clip (cookie-cutter) feature! # dissolve london_outline &lt;- london_wards %&gt;% summarise(area = sum(HECTARES)) # inspect tm_shape(london_outline) + tm_polygons() 2.4.3 Subsetting Now we have our London outline, we can go ahead and clip our schools data set by our London outline. Whilst there is a clip function within the sf library, what we will do here is use a techinque known as spatial subsetting, which is more similar to selecting by location: we will subset our london schools data set by filtering out those that are not within the London Outline. This approach in R is much quicker than using the clip function - although deciding which approach to use is not only a question of speed but also how each function will affect the filtered data. When using a clip function, the function acts exactly like a cookie-cutter and will trim off any data that overlaps with the boundaries used. Conversely, when using a subsetting approach, if a data point or polygon overlaps on the boundary, it will still be included (depending on the topological relationship used) but in its entirety (i.e. no trimming!). As we’re using point data, it is generally easier to use a subset approach. There are multiple ways and notations to conduct spatial subsetting within R: First, we can either use the [] notation just like you would use for selecting and slicing a normal (table-based) dataframe from R’s base package. Second, sf has its own named functions for geometric operations, including: intersection, difference, symmetrical difference and snap. What actually happens is that when you use the [] notation on the background one of those sf named functions get called. More details on manipulating simple features can be found in this vignette. To keep things simple, we will use the base subsetting approach - which also works similarly when programming in Python, for instance. # subset London schools london_schools_prj_ss &lt;- london_schools_prj[london_outline,] Note In a case like above, you can just overwrite the current london_schools_prj variable as you know it is the data set you want to use. Much of this code could be condensed into several lines using pipes to make our code shorter and more efficient - but then it would be harder to explain! As you progress with R and programming, you are welcome to bring pipes and restructuring into own your code - but even if you don’t, as long as your code does what you need it to do, then that’s our main aim with this course! Once you have run the above code, you should notice that your london_schools_prj_ss variable now only contains 3,372 records, instead of the original 3,889. We can also plot our variable using the same code as above, to double-check that it worked: # inspect tm_shape(london_wards) + tm_polygons() + tm_shape(london_schools_prj_ss) + tm_dots() We should now see that our schools are all contained within our ward data set, so we know this data set is ready to be used for analysis. We will now explore which schools are within 400m of greenspace and which are not. But first, we need to get our greenspace data ready so we can create the 400m buffers needed for this analysis. 2.4.4 Unioning We’ve done a lot of processing so far to do with our schools and ward data, but now it’s time for the greenspace data sets. If you look back at your code, you should remember that we have two data sets for our greenspace in London, which we now need to join together. This type of join is typically known as a union - and this is the type of tool you would want to look for across any GUI system. When it comes to programming, however, in either R or python, there is a much simpler way of joining data sets - and that’s simply copying over the records or observations from one variable into another - and the base library has a ready-to-go function for us to use, known as rbind(). This function allows you to ‘bind’ rows from one or more data sets together. This also works for sf objects. # join greenspace data sets together greenspace = rbind(TQ_greenspace, TL_greenspace) 2.4.5 Clipping The next step is to clip our reduced greenspace data to our London outline. Within sf, the clip function is known as the st_intersection() function - not to be confused with st_intersects() from above! A clip will change the geometry of some of our greenspaces on the outskirts of London, i.e. cookie-cut them precisely to the London outline. If we used the subset approach approach as we did earlier with our point data, we would simply extract all greenspaces that intersect with the London outline - but not change their geometry. What we can do however if reduce the processing required by our computer by using a mixture of these two methods - if we first subset our all_greenspace data set by our London outline and then run the clip, our processing will be much faster: # subset and clip london_greenspace &lt;- greenspace[london_outline,] %&gt;% st_intersection(london_outline) # inspect tm_shape(london_outline) + tm_polygons() + tm_shape(london_greenspace) + tm_polygons() 2.4.6 Attribute selection Now we have only London greenspaces in our data set, the next step, is to reduce the number of greenspaces to only those bigger than 50,000 square meters. To do this, we will use another type of subsetting you’ve probably come across, which is attribute subsetting - by using a simple query to subset only records that have an area larger than 50,000 square metres. To do this, we’ll use the filter() function from the dplyr library we mentioned earlier as well as another function called set_units() which is from the unit library that you’ve loaded - but we haven’t yet discussed. The set_units() function allows us to assign units to numerical values we are using within our query, i.e. here, for our query to run, our value must be in square metres to match the unit of the area_m column. To be able to query on our area, we must first calculate the area of each of our greenspaces. To do so in R, we can use the st_area() function within sf, which will calculate the area of each of our records/observations in our greenspace data set. To store the output of this function as a new column in our london_greenspace data set, we use a simple notation at the end of our london_greenspace variable: $area_m. The $ in R means for this data frame, access the column that proceeds this sign. In our case, we do not as yet have a column called area_m, therefore R will automatically create this column and then store the outputs of the function in this column: # calculate area london_greenspace$area_m &lt;- st_area(london_greenspace) Once we have our area column, we can now filter our data set based on that column and filter out all greenspace with an area that is smaller than 50,000 square meters. # filter large greenspaces large_london_greenspace &lt;- london_greenspace %&gt;% filter(area_m &gt; set_units(50000.0, m^2)) We now can look at our final greenspace data set against our london outline to see its final coverage: # inspect tm_shape(london_outline) + tm_polygons() + tm_shape(large_london_greenspace) + tm_polygons() 2.4.7 Buffering We now have our London greenspace data set - we are ready for the last step of processing with this data set - generating our buffers that we can use to find all schools within 400 meters of the large greenspace areas. Once again, the sf library has a function for generating buffers - we just need to know how to deploy it successfully on our London greenspace data set - and this involves understanding how to denote our distance correctly - as well as understanding if and how we can dissolve our buffer into a single record. To do this, we would investigate the documentation of the function st_buffer() to find out what additional parameters it takes - and how. What we can find out is that we need to (of course!) provide a distance for our buffer - but whatever figure we supply, this will be interpreted within the units of the CRS we are using. In our case, we are using British National Grid and, luckily for us, the units of the CRS is metres - which makes are life significantly easier when calculating these buffers. For other CRS, many use a base unit of an Arc Degree, e.g. WGS84. In this case, you technically have two options: 1) reproject your data into a CRS that uses metres as its base unit OR 2) convert your distance into an Arc Degree measurement. Always choose Option 1. Fortunately none of this is our concern - we know we can simply input the figure of 400 into our buffer and this will generate a buffer of 400m. # greenspace buffer gs_buffer_400m &lt;- st_buffer(large_london_greenspace, dist=400) As our final bit of processing with our greenspace buffer, we want to dissolve the whole buffer into a single record. To do this, we’ll replicate the code used for our London ward dissolve, creating a an area value for our buffer records in the process to be used within the summarisation - and then result in a new gs_buffer_400m_single variable: # dissolve greenspace buffer gs_buffer_400m_single &lt;- gs_buffer_400m %&gt;% summarise(area = sum(st_area(gs_buffer_400m))) # inspect tm_shape(london_outline) + tm_polygons() + tm_shape(gs_buffer_400m_single) + tm_polygons() 2.5 Greenspace in London Great, we are now ready to bring our two data sets together ready for analysis - and to do so, we’ll use subsetting as well as the st_intersects() function, although with this one, we’ll use it in two different ways! Our first task is to identify those schools that have access to greenspace - and extract them to create a new variable for use within our final point-in-polygon count (i.e. how many schools within each ward has access to greenspace). As we know, we can subset our london_schools data set by our greenspace buffer quite easily using the subset approach: # schools within 400m of greenspace london_schools_gs &lt;- london_schools_prj_ss[gs_buffer_400m_single,] Our london_schools_gs variable has been subsetted correctly if we end up with 1,770 records, instead of the 3,372 records we had previously. We can now use this data set and our previous london_schools_prj_ss data set to create counts at the ward level. But before we do that, we will create a binary attribute of greenspace access within our london_schools_prj_ss variable to visualise our school ‘points’. To do this, we’ll use the st_intersects() function mentioned above and add a new column, gs_access (i.e. greenspace access), which will tell us which schools have access to greenspace or not. The st_intersects() function is really useful as its output is a simple TRUE or FALSE statement - does this record intersect with the greenspace buffer? This result is what will be stored in our new column as a TRUE or FALSE response and what we can use to map our schools and their greenspace access: # greenspace access london_schools_prj_ss$gs_access &lt;- st_intersects(london_schools_prj_ss, gs_buffer_400m_single, sparse=FALSE) We could go ahead and recode this to create a 1 or 0, or YES or NO after processing, but for now we’ll leave it as TRUE or FALSE. We can go head and now visualise our schools based on this column, to see if they have access (TRUE) or do not have access (FALSE) to greenspace. To do this, we’ll use the tmap library again: # inspect tm_shape(london_schools_prj_ss) + tm_dots(col=&#39;gs_access&#39;, palette=&#39;BuGn&#39;) You’ll be pleased to read that we are finally here - we are at the last stage of our processing and can finally create the ward-level percentage of schools that have greenspace access, versus those that do not! To do this, we’ll be counting the number of points in each of our polygons, i.e. the number of schools in each ward. To do so in R and with sf, it is one line of code - which at first look does not sound at all like it is completing a point-in-polygon calculation - but it does! To create a PIP count within sf, we use the st_intersects() function again - but instead of using the output of TRUE or FALSE, what we actually extract from our function is its lengths recording. The lengths part of this function records how many times a join feature (i.e. our schools) intersects with our main features (i.e. our wards). (Note here, we do not set the sparse function to FALSE but leave it as TRUE/its default by not entering the parameter). As a result, the length of this list is equal to the count of how many schools are within the polygon - i.e. a PIP calculation. This is a really simple way of doing a PIP calculation - and makes it easy for us to store the output of the function and its lengths (and thus the count) directly as a column within our london_wards data set, as so: # total number of schools in each ward london_wards$total_schools &lt;- lengths(st_intersects(london_wards, london_schools_prj_ss)) # total number of schools with greenspace access in each ward london_wards$gs_schools &lt;- lengths(st_intersects(london_wards, london_schools_gs)) As you can see from the code above, we’ve now calculated this for our total schools data set and the schools that have access to greenspace. The final step in our processing therefore is to create our rate. To do so, we’ll use the same approach of generating a new column within our london_wards data set - and then use a mathematical formula to calculate our rates: # percentage of schools with greenspace access london_wards$gs_rate &lt;- (london_wards$gs_schools/london_wards$total_schools)*100 And that’s it! We now have our greenspace rate for our wards, which we can now again map: # inspect tm_shape(london_wards) + tm_polygons(col=&#39;gs_rate&#39;, palette=&#39;Greens&#39;) We now have our final data set ready for analysis. Right now, we haven’t introduced you to any statistical or spatial analysis techniques to fully analyse our data set - but instead, we can focus on what are data shows visually! The last step of any programming is to extract our variables into permanent data sets for use at a later time. You can at any point in this practical, extract a permanent data file for each of our variables. For now, we’ll extract our new london_wards data set as we might want to use this in some additional analysis that we could look at next week or for our assessments at a later stage. The great thing about coding this up now, is that it will be easy to re-run all of this analysis and export any of the variables, again, at a later time! # write st_write(obj=london_wards, dsn=&#39;london_ward_gs.shp&#39;, delete_dsn=TRUE) You should now see the data set appear in your files! 2.6 Attributions This week’s practical uses content and inspiration from: Wilkin, J. 2020. Analysing school access to greenspace in London. [Source] Dijk, J.V. 2021. Spatial Operations. [Source] 2.7 References (see reading list) Gitbook: [R Programming] Gimond, M. (2021) Intro to GIS and spatial analysis. Chapter 8: Spatial operations and vector overlays Click link Book: [Theory] Longley, P. et al (2015) Geographic Information Science &amp; systems, Chapter 13: Spatial Analysis Click link Gitbook: [R Programming] Lovelace, R., Nowosad, J. and Muenchow, J. (2021) Geocomputation with R, Chapter 4: Spatial data operations Click link Gitbook: [R Programming] Lovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 5: Geometry operations Click link Paper: [Research] Bijnens, E. et al (2020). Residential green space and child intelligence and behavior across urban, suburban, and rural areas in Belgium: A longitudinal birth cohort study of twins. PLOS Medicine 17(8), e1003213. Click link 2.8 Data Sources UK Shape files for different geographic units [Source: Office for National Statistics, Open Geography Portal] Click Here UK OS Greenspace open data [Source: Ordinance Survey] Click Here "],["spatial-autocorrelation.html", "3 Spatial autocorrelation 3.1 Introduction 3.2 Case study 3.3 Attributions 3.4 References (see reading list)", " 3 Spatial autocorrelation 3.1 Introduction This week, we focus on the first of two key properties of spatial data: spatial dependence. Spatial dependence is the idea that the observed value of a variable in one location is often dependent (to some degree) on the observed value of the same value in a nearby location. For spatial analysis, this dependence can be assessed and measured statistically by considering the level of spatial autocorrelation between values of a specific variable, observed in either different locations or between pairs of variables observed at the same location. Spatial autocorrelation occurs when these values are not independent of one another and instead cluster together across geographic space. A critical first step of spatial autocorrelation is to define the criteria under which a spatial unit (e.g. an areal or point unit) can be understood as a “neighbour” to another unit. As highlighted in previous weeks, spatial properties can often take on several meanings, and as a result, have an impact on the validity and accuracy of spatial analysis. This multiplicity also can be applied to the concept of spatial neighbours which can be defined through adjacency, contiguity or distance-based measures. As the specification of these criteria can impact the results, the definition followed therefore need to be grounded in particular theory that aims to represent the process and variable investigated. 3.1.1 Learning outcomes Understanding the notion that spatial dependence refers to the degree of spatial autocorrelation between independently measured values observed in geographical space. We will learn how to estimate global measures of spatial autocorrelation (e.g., Moran’s I) to provide a singular measure of spatial dependence. We will learn how Local indicators of spatial association (LISA) to perform a similar function but yield multiple location-specific measures of spatial dependence. 3.1.2 Getting started To enable the efficient, repeatable and reproducible functionality of our work, we will use R-Studio’s ability to create and host code as a script. Before we do anything therefore, we will need to create a new R script: File &gt; New File &gt; R Script. Remember, you can save your script with the quick save shortcuts (e.g. cmd + s (mac) / ctrl + s (windows)). 3.1.3 Datasets &amp; setting up the work directory Before you begin, if you have not done so already, please make sure to download all data by clicking here. Create a sub folder called “Week 3” within your “GEOG0114” folder stored in the desktop of your personal computer. Make sure to extract all data from the downloaded zip folder and store it into “Week 3” folder. Use your newly open R script and set the work directory to Week 3’s folder. For Windows, the code for setting the work directory will be: setwd(&quot;C:/Users/AccountName/Desktop/GEOG0114/Week 3&quot;) For MAC, the code for setting the work directory will be: setwd(&quot;/Users/AccountName/Desktop/GEOG0114/Week 3&quot;) 3.1.4 Loading and installing packages We will need to load the following packages from previous practicals: sf: Simple Features tmap: Thematic Mapping tidyverse: Contains a collection of packages to support efficient data managing The above packages sf, tmap and tidyverse should have been installed previously in the last session. Therefore, we do not have to install them again, we only need to activate them using the library() function. There will be some new packages we need to install: nngeo: The nngeo package includes functions for spatial join of layers based on k-nearest neighbor relation between features spdep: Provides access to a collection of functions to create spatial weights matrix objects from polygon contiguities, and testing spatial dependence sp: Provides access to a collection of functions for handling different classes and methods for spatial data data.table: Gives us access to function that deals with faster aggregation of large data frames etc., # install the packages using the install.package() install.packages(&quot;nngeo&quot;) install.packages(&quot;spdep&quot;) install.packages(&quot;sp&quot;) install.packages(&quot;data.table&quot;) # Load the packages with library() library(&quot;tidyverse&quot;) library(&quot;sf&quot;) library(&quot;tmap&quot;) library(&quot;nngeo&quot;) library(&quot;spdep&quot;) library(&quot;sp&quot;) library(&quot;data.table&quot;) 3.2 Case study This week looks at spatial dependence and autocorrelation in detail, focusing on the different methods of assessment. As part of this, we look at the multiple methods to defining spatial neighbours and their suitability of use across different spatial phenomena – and how this approach is used to generate spatial weights for use within these spatial autocorrelation methods as well as their potential to generate spatially-explicit variables. We put these learnings into practice through an analysis of spatial dependence of areal crime data, experimenting with the deployment of different neighbours and the impact of their analyses. For this practical we will look at the distribution of thefts from persons in the borough of Camden. 3.2.1 Neighbours If we want to come up with quantifiable descriptions of variables and how they vary over space, then we need to find ways of quantifying the distance from point to point. When you attach values to the polygons of wards in London, and visualise them, different patterns appear, and the different shapes and sizes of the polygons effect what these patterns look like. There can appear to be clusters, or the distribution can be random. If you want to explain and discuss variables, the underlying causes, and the possible solutions to issues, it becomes useful to quantify how clustered, or at the opposite end, how random these distributions are. This issue is known as spatial autocorrelation. In raster data, variables are measured at regular spatial intervals (or interpolated to be represented as such). Each measurement is regularly spaced from its neighbours, like the pixels on the screen you are reading this from. With vector data, the distance of measurement to measurement, and the size and shape of the “pixel” of measurement becomes part of the captured information. Whilst this can allow for more nuanced representations of spatial phenomena, it also means that the quantity and type of distance between measurements needs to be acknowledged. If you want to calculate the relative spatial relation of distributions, knowledge of what counts as a “neighbour” becomes useful. Neighbours can be neighbours due to euclidean distance (distance in space), or they can be due to shared relationships, like a shared boundary, or they can simply be the nearest neighbour, if there aren’t many other vectors around. Depending on the variable you are measuring the appropriateness of neighbourhood calculation techniques can change. 3.2.1.1 Loading our data sets Now we have the data in the correct folders, we can load and plot the shape data. # load Camden boundaries camden_oas &lt;- st_read(&#39;OAs_camden_2011.shp&#39;, crs=27700) # inspect tm_shape(camden_oas) + tm_polygons() You can see how one of these output areas could have many more neighbours than others, they vary a great deal in size and shape. The dimensions of these objects change over space, as a result the measurements within them must change too. Output areas are designed to convey and contain census information, so they are created in a way that maintains a similar number of residents in each one. The more sparsely populated an OA the larger it is. Output Areas are designed to cover the entirety of the land of England and Wales so they stretch over places where there are no people. In the north of Camden the largest Ouput Areas span over Hampstead Heath, a large park. Let’s explore how to find different kinds of neighbours using the example of one ‘randomly’ selected output area (E00004174) that happens to contain the UCL main campus. # highlight E00004174 tm_shape(camden_oas) + tm_borders(col=&#39;black&#39;) + tm_shape(camden_oas[camden_oas$OA11CD==&#39;E00004174&#39;,]) + tm_fill(col=&#39;red&#39;) 3.2.1.2 Euclidean neighbours The first way we are going to call something a neighbour is by using Euclidean distance. As our OA shapefile is projected in BNG (British National Grid), the coordinates are planar, going up 1 is the same distance as going sideways 1. Even better the coordinates are in metric measurements so it’s easy to make up heuristic distances. Let’s call every output area with a centroid 500m or less away from the centroid of our chosen OA a neighbour: we select only the the centroid of our chosen output area and all other areas (with st_centroid()) we set the maximum number of neighbours we want to find to “50” (with parameter k) we set the maximum distance of calling an OA centroid a neigbour to “500” (with parameter maxdist) we return a sparse matrix that tells us whether each OA is a neighbour or not (with parameter sparse) # assign our chosen OA to a variable chosen_oa &lt;- &#39;E00004174&#39; # identify neighbours chosen_oa_neighbours &lt;- st_nn(st_geometry(st_centroid(camden_oas[camden_oas$OA11CD==chosen_oa,])), st_geometry(st_centroid(camden_oas)), sparse = TRUE, k = 50, maxdist = 500) # inspect class(chosen_oa_neighbours) # get the names (codes) of these neighbours neighbour_names &lt;- camden_oas[chosen_oa_neighbours[[1]],] neighbour_names &lt;- neighbour_names$OA11CD # inspect tm_shape(camden_oas) + tm_borders() + # highlight only the neighbours tm_shape(camden_oas[camden_oas$OA11CD %in% neighbour_names,]) + tm_fill(col = &#39;green&#39;) + # highlight only the chosen OA tm_shape(camden_oas[camden_oas$OA11CD==chosen_oa,]) + tm_fill(col = &#39;red&#39;) + tm_shape(camden_oas) + # overlay the borders tm_borders(col=&#39;black&#39;) 3.2.1.3 Shared boundary neighbours The next way of calculating neighbours takes into account the actual shape and location of the polygons in our shapefile. This has only recently been added to the world of sf(), previously we would have reverted to using the sp() package and others that depend on it such as spdep(). We can create two functions that check whether any polygons share boundaries or overlap one another, and then also check by how much. These new functions are based on the st_relate() function. The different cases of these are known as queen, and rook. These describe the relations in a similar way to the possible chess board movements of these pieces. Note Do have a look at the short lectures by Luc Anselin on Moran’s I, the interpretation of Moran’s I, and neighbours and spatial weights for some additional explanation on measuring spatial autocorrelation with Moran’s I. # for rook case st_rook = function(a, b = a) st_relate(a, b, pattern = &#39;F***1****&#39;) # for queen case st_queen &lt;- function(a, b = a) st_relate(a, b, pattern = &#39;F***T****&#39;) Now that we’ve created the functions lets try them out. # identify neighbours chosen_oa_neighbours &lt;- st_rook(st_geometry(camden_oas[camden_oas$OA11CD==chosen_oa,]), st_geometry(camden_oas)) # get the names (codes) of these neighbours neighbour_names &lt;- camden_oas[chosen_oa_neighbours[[1]],] neighbour_names &lt;- neighbour_names$OA11CD # inspect tm_shape(camden_oas) + tm_borders() + # highlight only the neighbours tm_shape(camden_oas[camden_oas$OA11CD %in% neighbour_names,]) + tm_fill(col = &#39;green&#39;) + tm_shape(camden_oas[camden_oas$OA11CD==chosen_oa,]) + # highlight only the chosen OA tm_fill(col = &#39;red&#39;) + tm_shape(camden_oas) + # overlay the borders tm_borders(col=&#39;black&#39;) Note Because the tolerance of the shared boundaries in the st_rook() pattern and the st_queen() pattern, in this example they both assign the same neighbours. This is true for many non-square polygons as the difference is often given as whether two shapes share one or more points. Therefore the difference can have more to do with the resolution and alignment of your polygons than the actual spatial properties they represent. They can and do find different neighbours in other situations. Follow the grid example in the st_relate() documentation if you want to see it working. 3.2.2 Theft in Camden Now that we have found the different ways of finding neighbours we can consider how they relate to one another. There are two ways of looking at spatial autocorrelation: Global: This is a way of creating a metric of how regularly or irregularly clustered the variables are over the entire area studied. Local: This is the difference between an area and its neighbours. You would expect neighbours to be similar, but you can find exceptional places and results by seeing if places are quantifiably more like or dislike their neighbours than the average other place. But before we start that let’s get into the data we are going to use! We’ll be using personal theft data from around Camden. Our neighbourhood analysis of spatial autocorrelation should allow us to quantify the pattern of distribution of reported theft from persons in Camden in 2019. # load theft data camden_theft &lt;- read.csv(&#39;2019_camden_theft_from_person.csv&#39;) # convert csv to sf object camden_theft &lt;- st_as_sf(camden_theft, coords = c(&#39;X&#39;,&#39;Y&#39;), crs = 27700) # inspect tm_shape(camden_oas) + tm_polygons() + tm_shape(camden_theft) + tm_dots() This is point data, but we are interested in the polygons and how this data relates to the administrative boundaries it is within. Let’s count the number of thefts in each OA. This is a spatial operation that is often called “point in polygon”. As we are just counting the number of occurrences in each polygon it is quite easy. In the future you may often want to aggregate over points for an area, or in reverse assign values from the polygon to the points. # thefts in Camden camden_oas$n_thefts &lt;- lengths(st_intersects(camden_oas, camden_theft)) # inspect tm_shape(camden_oas) + tm_fill(col=&#39;n_thefts&#39;) + tm_borders(lty=&quot;solid&quot;) You can see our map is skewed by central London, meaning that the results in central London (the south of Camden) are so much larger than those in the north that it makes it harder to see the smaller differences between other areas. We’ll take the square root of the number of thefts to remedy this. # square root of thefts camden_oas$sqrt_n_thefts &lt;- sqrt(camden_oas$n_thefts) # inspect tm_shape(camden_oas) + tm_fill(col=&#39;sqrt_n_thefts&#39;) + tm_borders(lty=&quot;solid&quot;) There: a slightly more nuanced picture 3.2.3 Global Moran’s I With a Global Moran’s I we can test how “random” the spatial distribution of these values is. Global Moran’s I is a metric between -1 and 1. -1 is a completely even spatial distribution of values, 0 is a “random” distribution, and 1 is a “non-random” distribution of clearly defined clusters. To calculate the Global Moran’s I you need an adjacency matrix that contains the information of whether or not an OA is next to another. For an even more nuanced view you can include distance, or a distance weighting in the matrix rather than just the TRUE or FALSE, to take into account the strength of the neighbourhoodness. Because of the way Moran’s I functions in R it is necessary to use the sp and spdep libraries (which we have loaded earlier). As you will see these methods and functions have quite esoteric and complicated syntax. Some of the operations they will do will be similar to the examples shown earlier, but the way they assign and store variables makes it much quicker to run complex spatial operations. # inspect class(camden_oas) ## [1] &quot;sf&quot; &quot;data.frame&quot; # convert to sp camden_oas_sp &lt;- as_Spatial(camden_oas, IDs=camden_oas$OA11CD) # inspect class(camden_oas_sp) ## [1] &quot;SpatialPolygonsDataFrame&quot; ## attr(,&quot;package&quot;) ## [1] &quot;sp&quot; Now we can make the esoteric and timesaving “nb” object in which we store for each OA which other OAs are considered to be neighbours. # create an nb object camden_oas_nb &lt;- poly2nb(camden_oas_sp, row.names=camden_oas_sp$OA11CD) # inspect class(camden_oas_nb) ## [1] &quot;nb&quot; # inspect str(camden_oas_nb,list.len=10) ## List of 749 ## $ : int [1:7] 10 15 215 303 327 375 464 ## $ : int [1:5] 19 72 309 365 430 ## $ : int [1:3] 133 152 709 ## $ : int [1:7] 78 131 152 286 314 582 651 ## $ : int [1:5] 67 316 486 492 703 ## $ : int [1:8] 7 68 317 487 556 612 625 638 ## $ : int [1:3] 6 68 317 ## $ : int [1:7] 57 58 164 358 429 605 684 ## $ : int [1:5] 58 164 489 609 700 ## $ : int [1:7] 1 215 245 311 327 366 644 ## [list output truncated] ## - attr(*, &quot;class&quot;)= chr &quot;nb&quot; ## - attr(*, &quot;region.id&quot;)= chr [1:749] &quot;E00004395&quot; &quot;E00004314&quot; &quot;E00004578&quot; &quot;E00004579&quot; ... ## - attr(*, &quot;call&quot;)= language poly2nb(pl = camden_oas_sp, row.names = camden_oas_sp$OA11CD) ## - attr(*, &quot;type&quot;)= chr &quot;queen&quot; ## - attr(*, &quot;sym&quot;)= logi TRUE Next, we need to assign weights to each neighbouring polygon. In our case, each neighbouring polygon will be assigned equal weight with style='W'. After this, we can calculate a value for the Global Moran’s I. # create the list weights object nb_weights_list &lt;- nb2listw(camden_oas_nb, style=&#39;W&#39;) # inspect class(nb_weights_list) ## [1] &quot;listw&quot; &quot;nb&quot; # Moran&#39;s I mi_value &lt;- moran(camden_oas_sp$n_thefts,nb_weights_list,n=length(nb_weights_list$neighbours),S0=Szero(nb_weights_list)) # inspect mi_value ## $I ## [1] 0.4772137 ## ## $K ## [1] 75.21583 The Global Moran’s I seems to indicate that there is indeed some spatial autocorrelation in our data, however, this is just a quick way to check the score. To do so properly we need to compare our score a randomly distributed version of the variables. We can do this by using something called a Monte Carlo simulation. # run a Monte Carlo simulation 599 times mc_model &lt;- moran.mc(camden_oas_sp$n_thefts, nb_weights_list, nsim=599) # inspect mc_model ## Monte-Carlo simulation of Moran I ## ## data: camden_oas_sp$n_thefts ## weights: nb_weights_list ## number of simulations + 1: 600 ## ## statistic = 0.47721, observed rank = 600, p-value = 0.001667 ## alternative hypothesis: greater This model shows that our distribution of thefts differs significantly from a random distribution. As such, we can conclude that there is significant spatial autocorrelation in our theft data set. 3.2.4 Local Moran’s I (or LISA) With a measurement of local spatial autocorrelation we could find hotspots of theft that are surrounded by areas of much lower theft. According to the previous global statistic these are not randomly distributed pockets but would be outliers against the general trend of clusteredness! These could be areas that contain very specific locations, where interventions could be made that drastically reduce the rate of crime rather than other areas where there is a high level of ambient crime. # create an nb object camden_oas_nb &lt;- poly2nb(camden_oas_sp, row.names=camden_oas_sp$OA11CD) # create the list weights object nb_weights_list &lt;- nb2listw(camden_oas_nb, style=&#39;W&#39;) # Local Moran&#39;s I local_moran_camden_oa_theft &lt;- localmoran(camden_oas_sp$n_thefts, nb_weights_list) To properly utilise these local statistics and make an intuitively useful map, we need to combine them with our crime count variable. Because of the way the new variable will be calculated, we first need to rescale our variable so that the mean is 0. # rescale camden_oas_sp$scale_n_thefts &lt;- scale(camden_oas_sp$n_thefts) To compare this rescaled value against its neighbours, we subsequently need to create a new column that carries information about the neighbours. This is called a spatial lag function. The “lag” just refers to the fact you are comparing one observation against another, this can also be used between timed observations. In this case, the “lag” we are looking at is between neighbours. # create a spatial lag variable camden_oas_sp$lag_scale_n_thefts &lt;- lag.listw(nb_weights_list, camden_oas_sp$scale_n_thefts) Now we have used sp for all it is worth it’s time to head back to the safety of sf() before exploring any forms of more localised patterns. # convert to sf camden_oas_moran_stats &lt;- st_as_sf(camden_oas_sp) To make a human readable version of the map we will generate some labels for our findings from the Local Moran’s I stats. This process calculates what the value of each polygon is compared to its neighbours and works out if they are similar or dissimilar and in which way, then gives them a text label to describe the relationship. # set a significance value sig_level &lt;- 0.1 # classification with significance value camden_oas_moran_stats$quad_sig &lt;- ifelse(camden_oas_moran_stats$scale_n_thefts &gt; 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &gt; 0 &amp; local_moran_camden_oa_theft[,5] &lt;= sig_level, &#39;high-high&#39;, ifelse(camden_oas_moran_stats$scale_n_thefts &lt;= 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &lt;= 0 &amp; local_moran_camden_oa_theft[,5] &lt;= sig_level, &#39;low-low&#39;, ifelse(camden_oas_moran_stats$scale_n_thefts &gt; 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &lt;= 0 &amp; local_moran_camden_oa_theft[,5] &lt;= sig_level, &#39;high-low&#39;, ifelse(camden_oas_moran_stats$scale_n_thefts &lt;= 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &gt; 0 &amp; local_moran_camden_oa_theft[,5] &lt;= sig_level, &#39;low-high&#39;, ifelse(local_moran_camden_oa_theft[,5] &gt; sig_level, &#39;not-significant&#39;, &#39;not-significant&#39;))))) # classification without significance value camden_oas_moran_stats$quad_non_sig &lt;- ifelse(camden_oas_moran_stats$scale_n_thefts &gt; 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &gt; 0, &#39;high-high&#39;, ifelse(camden_oas_moran_stats$scale_n_thefts &lt;= 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &lt;= 0, &#39;low-low&#39;, ifelse(camden_oas_moran_stats$scale_n_thefts &gt; 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &lt;= 0, &#39;high-low&#39;, ifelse(camden_oas_moran_stats$scale_n_thefts &lt;= 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &gt; 0, &#39;low-high&#39;,NA)))) To understand how this is working we can look at the data non-spatially. As we rescaled the data, our axes should split the data neatly into their different area vs spatial lag relationship categories. Let’s make the scatterplot using the scaled number of thefts for the areas in the x axis and their spatially lagged results in the y axis. # plot 1 # plot the results without the statistical significance ggplot(camden_oas_moran_stats, aes(x = scale_n_thefts, y = lag_scale_n_thefts, color = quad_non_sig)) + geom_vline(xintercept = 0) + # plot vertical line geom_hline(yintercept = 0) + # plot horizontal line xlab(&#39;Scaled Thefts (n)&#39;) + ylab(&#39;Lagged Scaled Thefts (n)&#39;) + labs(colour=&#39;Relative to neighbours&#39;) + geom_point() # plot 2 # plot the results with the statistical significance ggplot(camden_oas_moran_stats, aes(x = scale_n_thefts, y = lag_scale_n_thefts, color = quad_sig)) + geom_vline(xintercept = 0) + # plot vertical line geom_hline(yintercept = 0) + # plot horizontal line xlab(&#39;Scaled Thefts (n)&#39;) + ylab(&#39;Lagged Scaled Thefts (n)&#39;) + labs(colour=&#39;Relative to neighbours&#39;) + geom_point() Now let’s see how they are arranged spatially. # map all of the results here tm_shape(camden_oas_moran_stats) + tm_fill(col = &#39;quad_non_sig&#39;, palette = c(&quot;#de2d26&quot;, &quot;#fee0d2&quot;, &quot;#deebf7&quot;, &quot;#3182bd&quot;)) # map only the statistically significant results here # map only the statistically significant results here tm_shape(camden_oas_moran_stats) + tm_fill(col = &#39;quad_sig&#39;, palette = c(&quot;#de2d26&quot;, &quot;#fee0d2&quot;, &quot;white&quot;)) + tm_borders(col = &quot;grey&quot;) As our data are so spatially clustered we can’t see any outlier places (once we have ignored the non-significant results). This suggests that the pattern of theft from persons is not highly concentrated in very small areas or particular Output Areas, and instead is spread on a larger scale than we have used here. To go further than we have today it would be possible to run the exact same code but using a larger scale, perhaps LSOA, or Ward, and compare how this changes the Moran’s I statistics globally and locally. Or, to gain statistical significance in looking at the difference between areas getting more data perhaps over a longer timescale, where there are less areas with 0 thefts. 3.3 Attributions This week’s practical uses content and inspiration from: Long, A. 2020. Spatial autocorrelation. [Source] Dijk, J.V. 2021. Spatial autocorrelation. [Source] Gimond, M. 2021. Spatial autocorrelation in R. [Source] 3.4 References (see reading list) Gitbook: [R Programming] Gimond, M. (2021) Intro to GIS and spatial analysis. Chapter 13: Spatial autocorrelation Click link Book: [Theory] Longley, P. et al (2015) Geographic Information Science &amp; systems (4th Edition); Chapter 2: The Nature of Geographic Data Click link Book: [Theory] Longley, P. _*et al_ (2015) Geographic Information Science &amp; systems (4th Edition); Chapter 13: Spatial Analysis Click link Paper: [Research] Radil, S. 2016. Spatial analysis of crime. In: Huebner, B. and Bynum, T. The Handbook of Measurement Issues in Criminology and Criminal Justice, Chapter 24, pp.536-554. Click link (Note: use of your UCL login via institutional organisation (UK Federation) is required to access the digital paper) "],["suitability-mapping-part-1.html", "4 Suitability Mapping: Part 1 4.1 Introduction 4.2 Suitability mapping using Simple Overlays 4.3 Analytical Hierarchy Process (AHP) 4.4 References (see reading list)", " 4 Suitability Mapping: Part 1 4.1 Introduction Now, with the proliferation of open spatial dataset, risk models derived from environmental, sociodemographic and topological factors are becoming increasingly available for open research. Such models have broadly shown to be useful in delineating geographical areas of risk or suitability for a certain outcomes. Today, we are going to explore this qualitatively using a knowledge-driven approach. 4.1.1 Learning outcomes To provide an introductory overview to the applicability of knowledge-driven methods, in particular, we are going to learn the Multi-Criteria Decision Approach (MCDA) which is a method that use decision rules from existing knowledge to identify areas potential suitability for an outcome. It is especially useful in data-sparse situations, or when for the first time exploring the potential geographical limits of certain outcome. For instance, using modest number of raster layers such as population density, urbanisation, approximating to street segments, house prices and deprivation; it is possible to combine such information so to determine regions for which crime events such as burglaries are likely to occur, or suitable in that matter. This approach has been widely used in a number of disciplines over the last decades, and has gained prominence in public health related fields such as vector-borne disease prevention, and disaster sciences such as landslides. We will learn how to apply these methods to the two context. 4.1.2 Datasets &amp; setting up the work directory Before you begin do make sure to download all data by clicking here. Create a folder on called “Week 4” within your “GEOG0114” folder stored in the desktop of your personal computer. Make sure to extract all data from the zip folder and store it into “Week 4” folder. Open a new R script and set the work directory to Week 4’s folder. For Windows, the work directory will be: setwd(&quot;C:/Users/AccountName/Desktop/GEOG0114/Week 4&quot;) For MAC, the work directory will be: setwd(&quot;/Users/AccountName/Desktop/GEOG0114/Week 4&quot;) 4.1.3 Loading and installing packages We will need to load the following packages: sf: Simple Features tmap: Thematic Mapping raster: Raster/gridded data analysis and manipulation sp: Package for providing classes for spatial data (points, lines, polygons and grids) The above packages sf, tmap, raster &amp; sp should have been installed in the previous session(s). We will need to install a new package: spatialEco - Spatial Analysis and Modelling Utilities package, which provides the user further functions such as raster.invert() which we will need in the third section. BAMMtools - It provides the user access to an important function getJenksBreaks() for the AHP analysis in the third section. # Install the packages: spatialEco using the install.package() install.packages(&quot;spatialEco&quot;) install.packages(&quot;BAMMtools&quot;) # Load the packages with library() library(&quot;sf&quot;) library(&quot;tmap&quot;) library(&quot;raster&quot;) library(&quot;sp&quot;) library(&quot;spatialEco&quot;) library(&quot;BAMMtools&quot;) 4.1.4 Loading datasets We will be dealing with raster data only for this exercise. These will be a series of climate, environmental and other social-anthropogenic gridded data from Kenya. We are going to combine them in order to determine areas that are suitable for disease transmission of Lymphatic Filariasis, a neglected tropical disease, caused by a range of mosquitoes that spread the parasitic worms called the Wuchereria bancrofti through a mosquito bite. There is evidence that indicates that precipitation, temperature, population density, aridity, dryness, land surface elevation and vegetation are risk factors for mosquito infestation, and in turn, greatly influencing the way mosquitoes breed and feeding behaviour within human dwellings thus maintaining disease transmission. We will use the following techniques for mapping areas of suitability: Binary Classification &amp; Ranking Suitability Mapping using Weighted Overlay Analysis MCDA Mapping Lets begin loading the following list of raster files, each is a variable of interest: Raster: Mean Temperature in Kenya (1000m resolution) named Kenya Mean Teperature.tif Raster: Normalized Difference Vegetation Index in Kenya (1000m resolution) named Kenya NDVI.tif Raster: Precipitation in Kenya (1000m resolution) named Kenya Precipitation.tif Raster: Population Density in Kenya (1000m resolution) named Kenya Population Density.tif Raster: Land Surface Elevation in Kenya (1000m resolution) named Kenya Elevation.tif Raster: Levels of Dryness in Kenya (1000m resolution) named Kenya Aridity Index.tif temp &lt;- raster(&quot;Kenya Mean Teperature.tif&quot;) nvdi &lt;- raster(&quot;Kenya NDVI.tif&quot;) prec &lt;- raster(&quot;Kenya Precipitation.tif&quot;) popl &lt;- raster(&quot;Kenya Population Density.tif&quot;) elev &lt;- raster(&quot;Kenya Elevation.tif&quot;) arid &lt;- raster(&quot;Kenya Aridity Index.tif&quot;) # ignore those stupid warning messages as these are annoying as hell and are related to same migration/updates done on PROJ4 to PROJ6 - which I can&#39;t for the life of me figure out how to implement this bloody update. Load the shapefile nation and state borders for Kenya: Shape file: Kenya’s National border named Kenya_Border_3857.shp Shape file: Kenya’s State border named Kenya_States_3857.shp # load the shapefiles kenya_border &lt;- st_read(&quot;Kenya_Border_3857.shp&quot;) kenya_states &lt;- st_read(&quot;Kenya_States_3857.shp&quot;) IMPORTANT NOTES: All shape file and raster data were in projected to the CRS: Spherical mercator 3857 You can inspect each raster to know its dimension, extent, resolution and minimum and maximum values. Since are going to stack all the rasters together, you definitely want the: dimension, extent and resolution to be the same. If there’s a slight difference the stack won’t work. # for instance temp and aridity temp arid To visualize raster data - you can write the following code: # For instance take the mean temperature for Kenya # Spectral colours are useful for diverging scales &quot;Spectral&quot; is Rd-Or-Yl-Gr-Bu. &quot;-Spectral&quot; reverses the order tm_shape(temp) + tm_raster(style = &quot;cont&quot;, title = &quot;Mean Temperature&quot;, palette= &quot;-Spectral&quot;) + tm_shape(kenya_states) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) 4.2 Suitability mapping using Simple Overlays 4.2.1 Setting up binary classification on raster One thing to know about this approach - the mappings are purely knowledge-driven and not analytical. For instance, according to previous studies (see example of papers: 1 &amp; 2), we have learnt a bit about the limits or thresholds for these predictors from or below which we can consider an area being suitable for the occurrence of Lymphatic Filariasis (LF). We have summarized these limits or thresholds in a table as follows: Risk factor Threshold for suitability Temperature &gt; 15.0 (degree Celsius) Precipitation &gt; 350 (mm of rainfall) Aridity &gt; 0.20 (i.e., semi-arid (&gt; 0.5) dry sub-humid) Elevation &lt; 1200m (low land) Population Density &gt; 0 (Inhabitants must exist source for blood meal) NDVI &gt; 0.5 (mild levels of vegetation) We should use the aforementioned thresholds to produce binary or Boolean maps using the above criteria. This means that the pixel values of each raster layer will be equal to 0 indicating that its an unsuitable condition for LF transmission, and 1 for suitable conditions for LF transmission. Let us reclassify each layer according to the above criteria, starting with temperature: # reclassify temperature as a binary or Boolean layer temp # lowest value = 1.2 # highest value = 29.6 # reclassify anything below 15 as 0 and above as 1 # take the minimum value and minus 1 to it to ensure that lowest # ---value is included in the categorization for 0 # take the maximum value and add 1 to it to also ensure that the highest # ---value is included in the categorization for 1 # c(from lowest value, to threshold, class as 0, from thresholds, to maximum value, class as 1) temp_cl &lt;- c(temp@data@min-1, 15, 0, 15, temp@data@max+1, 1) # convert into a matrix format temp_cl_mat &lt;- matrix(temp_cl, ncol = 3, byrow = TRUE) # see matrix temp_cl_mat # apply matrix to reclassify() function to categorize the raster accordingly temp_recl &lt;- reclassify(temp, temp_cl_mat) When you reclassify the raster for temp to temp_recl. This is what the output should look like: tm_shape(temp_recl) + tm_raster(style = &quot;cat&quot;, title = &quot;Temperature&quot;, palette= c(&quot;grey&quot;, &quot;#F1948A&quot;), labels = c(&quot;Unsuitable (&lt;15.0)&quot;, &quot;Suitable (15 &amp; above)&quot;)) + tm_shape(kenya_states) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) Lets repeat the same steps for elevation to reclassify it’s layer according to the given criteria: # reclassify elevation as a binary or Boolean layer elev # lowest value = -11m (below sea level) # highest value = 4865m (above sea level) # reclassify anything below 1200m as 1 and above as 0 # again, take the minimum value and minus 1 to it to # ---ensure that lowest value is included in the categorization for 0 # again, take the maximum value and add 1 to it to also ensure that # ---the highest value is included in the categorization for 1 # c(from lowest value, to threshold, class as 1, from thresholds, to maximum value, class as 0) elev_cl &lt;- c(elev@data@min-1, 1200, 1, 1200, elev@data@max+1, 0) # convert into a matrix format elev_cl_mat &lt;- matrix(elev_cl, ncol = 3, byrow = TRUE) # see matrix elev_cl_mat # apply matrix to reclassify() function to categorize the raster accordingly elev_recl &lt;- reclassify(elev, elev_cl_mat) The elevation output should look something like: tm_shape(elev_recl) + tm_raster(style = &quot;cat&quot;, title = &quot;Elevation&quot;, palette= c(&quot;grey&quot;, &quot;orange&quot;), labels = c(&quot;Unsuitable (&gt;1200m)&quot;, &quot;Suitable (1200m &amp; below)&quot;)) + tm_shape(kenya_states) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) Try reclassifying the remaining raster arid, prec, popl and ndvi. The solutions are provided in the code chunks below. Click here Solutions # nvdi nvdi nvdi_cl &lt;- c(nvdi@data@min-1, 0.5, 0, 0.5, nvdi@data@max+1, 1) nvdi_cl_mat &lt;- matrix(nvdi_cl, ncol = 3, byrow = TRUE); nvdi_cl_mat nvdi_recl &lt;- reclassify(nvdi, nvdi_cl_mat) # prec prec prec_cl &lt;- c(prec@data@min-1, 350, 0, 350, prec@data@max+1, 1) prec_cl_mat &lt;- matrix(prec_cl, ncol = 3, byrow = TRUE); prec_cl_mat prec_recl &lt;- reclassify(prec, prec_cl_mat) # popl popl popl_cl &lt;- c(popl@data@min-1, 0, 0, 0, popl@data@max+1, 1) popl_cl_mat &lt;- matrix(popl_cl, ncol = 3, byrow = TRUE) popl_cl_mat popl_recl &lt;- reclassify(popl , popl_cl_mat) # arid arid arid_cl &lt;- c(arid@data@min-1, 0.20, 0, 0.20, arid@data@max+1, 1) arid_cl_mat &lt;- matrix(arid_cl, ncol = 3, byrow = TRUE) arid_cl_mat arid_recl &lt;- reclassify(arid, arid_cl_mat) 4.2.2 Finding areas of suitability by multiplying the binary rasters We can identify the environmentally suitable areas for occurrence of LF by simply multiplying the binary maps. Therefore, only the cells or areas with the value of 1 will be kept in the output raster layer. You can do this by writing the following formula: Suitable_LF_Binary &lt;- temp_recl*nvdi_recl*prec_recl*elev_recl*popl_recl*arid_recl Visualizing the output: tm_shape(Suitable_LF_Binary) + tm_raster(style = &quot;cat&quot;, title = &quot;&quot;, palette=c(&quot;#f0f0f0&quot;, &quot;red&quot;), labels=c(&quot;Zone: Not Suitable&quot;, &quot;Zone: Highly Suitable&quot;)) + tm_shape(kenya_states) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) IMPORTANT NOTES: Red region corresponds to areas that are highly suitable for LF. South East of Kenya (i.e., Kwale, Kilifi and Lamu) are environmental suitable based on binary classification 4.2.3 Finding areas of suitability by summing the binary rasters We can identify the environmentally suitable areas for occurrence of LF by simply summing the binary maps generate ratings for each pixel to show the intensity of suitability for LF. A pixel with a summed value would mean the following: 0 would mean that none of the six factors are present at pixel. 1 would mean at least one out of the six factors are present at that pixel. 2 would mean at least two out of the six risk factors are present at that pixel (and so on…) 6 is the maximum meaning that all factors are present there. Therefore, pixel is rated as the highest levels of suitability for LF. To sum up the raster, we would first need to use the stack() function before using the calc() for the summation. RasterStack &lt;- stack(temp_recl, nvdi_recl, prec_recl, elev_recl, popl_recl, arid_recl) Suitable_LF_Summed &lt;- calc(RasterStack, sum) # check for minimum and maximum Suitable_LF_Summed@data@min Suitable_LF_Summed@data@max # minimum = 2 # maximum = 6 Visualizing the output: tm_shape(Suitable_LF_Summed) + tm_raster(style = &quot;cat&quot;, title = &quot;Suitability score&quot;, palette=c(&quot;#FDFEFE&quot;, &quot;#FADBD8&quot;, &quot;#F5B7B1&quot;, &quot;#F1948A&quot;, &quot;#E74C3C&quot;), labels=c(&quot;Low (2)&quot;, &quot;Modest (3)&quot;, &quot;Medium (4)&quot;, &quot;High (5)&quot;, &quot;Highest (6)&quot;)) + tm_shape(kenya_states) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_text(&quot;NAME_1&quot;, size = &quot;AREA&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) IMPORTANT NOTES: Using this approach allows the user to see of the fly the intensity for LF suitability. States such as Mandera, Wajir, Isiolo and Turkana have areas that have largely low or modest ratings for LF suitability. Whereas, again the highly suitable areas for LF are the South East states (i.e., Kwale, Kilifi and Lamu). The rating approach is much better because you can see the intensity for LF suitability. Both approaches highly accessible especially for data and public health program managers in sub-Saharan Africa. These kinds of maps are particularly useful to optimize resources (especially within a low-resource setting) for identifying high-risk areas through knowledge-driven approach (and averting to survey potential unsuitable areas to not wasting limited resources). This approach is great for descriptive knowledge-based decision analysis; however, it does not take into account the factor that certain variables have more importance than others. For instance, precipitation is significantly more important than NDVI because elevated levels of rainfall has a positive impact on a mosquitoes’ survival lifespan and it provides an reservoir for female mosquitoes to breed in standing water. Higher rainfall within a densely populated environment provides not only a breeding ground, but a location closer to their food source (i.e., human and animal blood). Thus, population density is important an factor above NDVI but not as important as precipitation (i.e., high importance: precipitation &gt; population density &gt; NDVI :less importance). We can take this into account by applying weights for each variable determined by importance for the MCDA process - this approach is referred to as the Saaty’s Analytical Hierarchy Process (AHP). Let us see how we apply this methodology. 4.3 Analytical Hierarchy Process (AHP) 4.3.1 Standardizing the raster variables to the same scale (from 1 to 10) We are going to use Saaty’s Analytical Hierarchy Process (AHP). We will need to standardize our raster factors in order to make comparisons and combination possible, as all of them contain different measures: temp (degree Celsius), prec (mm), elev (meters), popl (counts/sqkm); while nvdi and arid are derived indices without any units. Before deriving the weights and applying to the equation that is a linear combination of the above variables to estimate the suitability index for LF, we can standardize them using the Jenks Natural Breaks algorithm. IMPORTANT NOTES: Jenks Natural Breaks algorithm is the preferred approach because it calculates the optimum breakpoints while seeking to minimize the variance within categories, and at the same time maximizing the variance between categories. Let’s begin to standardize the first variable temp. There is a bit of data cleaning involved - here is the code for to calculate the breaks using Jenks algorithm to get the raster scales from 1 to 10. # cleaning for temp # Extract values from Raster tempValues &lt;- values(temp) # Change the values from vector object to data.frame object tempDF &lt;- as.data.frame(tempValues) # Remove missing values and reapply column name tempDF &lt;- as.data.frame(tempDF[!is.na(tempDF$tempValues),]) colnames(tempDF) &lt;- &quot;tempValues&quot; # Use the getJenksBreaks() function. Sample 0.10 (10%) of the pixels #---at random and base the categorisation on this. # NOTE: Doing this on the full data will take forever - so use the subset argument. tempJenks &lt;- getJenksBreaks(tempDF$tempValues, 10, subset = nrow(tempDF)*0.10) # See value in vector tempJenks # shows first element tempJenks[1] # shows second element tempJenks[2] # so on and so further... # Create categorisation by using the Jenks values in the vector temp_jenks_cl &lt;- c(temp@data@min-1, tempJenks[1], 1, tempJenks[1], tempJenks[2], 2, tempJenks[2], tempJenks[3], 3, tempJenks[3], tempJenks[4], 4, tempJenks[4], tempJenks[5], 5, tempJenks[5], tempJenks[6], 6, tempJenks[6], tempJenks[7], 7, tempJenks[7], tempJenks[8], 8, tempJenks[8], tempJenks[9], 9, tempJenks[9], temp@data@max+1, 10) # create matrix temp_jenks_cl_mat &lt;- matrix(temp_jenks_cl, ncol = 3, byrow = TRUE) # view categorisation in matrix temp_jenks_cl_mat # reclassify original raster using the jenks classifications temp_jenks_recl &lt;- reclassify(temp, temp_jenks_cl_mat) Visualize the output with the scale from 1 to 10: tm_shape(temp_jenks_recl) + tm_raster(style = &quot;cont&quot;, title = &quot;Temp (on Jenks scale)&quot;, palette= &quot;-Spectral&quot;) + tm_shape(kenya_states) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) We need to repeat this process of reclassification and standardization using natural breaks for the remaining raster grids for prec, arid, popl and ndvi. The solutions are provided in the hidden code chunks below. For elev, we will treat this differently. Click here Solutions # 2 prec # Extract values from Raster precValues &lt;- values(prec) # Change the values from vector object to data.frame object precDF &lt;- as.data.frame(precValues) # Remove missing values and reapply column name precDF &lt;- as.data.frame(precDF[!is.na(precDF$precValues),]) colnames(precDF) &lt;- &quot;precValues&quot; # Use the getJenksBreaks() function. Sample 0.10 (10%) of the pixels at random and base the categorisation on this. # NOTE: Doing this on the full data will take forever - so use the subset argument. precJenks &lt;- getJenksBreaks(precDF$precValues, 10, subset = nrow(precDF)*0.10) # See value in vector precJenks # shows first element precJenks[1] # shows second element precJenks[2] # so on and so further... # Create categorisation by using the Jenks values in the vector prec_jenks_cl &lt;- c(prec@data@min-1, precJenks[1], 1, precJenks[1], precJenks[2], 2, precJenks[2], precJenks[3], 3, precJenks[3], precJenks[4], 4, precJenks[4], precJenks[5], 5, precJenks[5], precJenks[6], 6, precJenks[6], precJenks[7], 7, precJenks[7], precJenks[8], 8, precJenks[8], precJenks[9], 9, precJenks[9], prec@data@max+1, 10) # create matrix prec_jenks_cl_mat &lt;- matrix(prec_jenks_cl, ncol = 3, byrow = TRUE) # view categorisation in matrix prec_jenks_cl_mat # reclassify original raster using the jenks classifications prec_jenks_recl &lt;- reclassify(prec, prec_jenks_cl_mat) # 3. popl # Extract values from Raster poplValues &lt;- values(popl) # Change the values from vector object to data.frame object poplDF &lt;- as.data.frame(poplValues) # Remove missing values and reapply column name poplDF &lt;- as.data.frame(poplDF[!is.na(poplDF$poplValues),]) colnames(poplDF) &lt;- &quot;poplValues&quot; # Use the getJenksBreaks() function. Sample 0.10 (10%) of the pixels at random and base the categorisation on this. # NOTE: Doing this on the full data will take forever - so use the subset argument. poplJenks &lt;- getJenksBreaks(poplDF$poplValues, 10, subset = nrow(poplDF)*0.10) # See value in vector poplJenks # shows first element poplJenks[1] # shows second element poplJenks[2] # so on and so further... # Create categorisation by using the Jenks values in the vector popl_jenks_cl &lt;- c(popl@data@min-1, poplJenks[1], 1, poplJenks[1], poplJenks[2], 2, poplJenks[2], poplJenks[3], 3, poplJenks[3], poplJenks[4], 4, poplJenks[4], poplJenks[5], 5, poplJenks[5], poplJenks[6], 6, poplJenks[6], poplJenks[7], 7, poplJenks[7], poplJenks[8], 8, poplJenks[8], poplJenks[9], 9, poplJenks[9], popl@data@max+1, 10) # create matrix popl_jenks_cl_mat &lt;- matrix(popl_jenks_cl, ncol = 3, byrow = TRUE) # view categorisation in matrix popl_jenks_cl_mat # reclassify original raster using the jenks classifications popl_jenks_recl &lt;- reclassify(popl, popl_jenks_cl_mat) # 4 nvdi # Extract values from Raster nvdiValues &lt;- values(nvdi) # Change the values from vector object to data.frame object nvdiDF &lt;- as.data.frame(nvdiValues) # Remove missing values and reapply column name nvdiDF &lt;- as.data.frame(nvdiDF[!is.na(nvdiDF$nvdiValues),]) colnames(nvdiDF) &lt;- &quot;nvdiValues&quot; # Use the getJenksBreaks() function. Sample 0.10 (10%) of the pixels at random and base the categorisation on this. # NOTE: Doing this on the full data will take forever - so use the subset argument. # EXTRA NOTE: The values for nvdi are very close to each other and so the algorithm splits it to just two cateogries nvdiJenks &lt;- getJenksBreaks(nvdiDF$tempValues, 2, subset = nrow(nvdiDF)*0.10) # See value in vector nvdiJenks # shows first element nvdiJenks[1] # shows second element nvdiJenks[2] # so on and so further... # Create categorisation by using the Jenks values in the vector nvdi_jenks_cl &lt;- c(nvdi@data@min-1, nvdiJenks[1], 1, nvdiJenks[1], nvdi@data@max+1, 2) # create matrix nvdi_jenks_cl_mat &lt;- matrix(nvdi_jenks_cl, ncol = 3, byrow = TRUE) # view categorisation in matrix nvdi_jenks_cl_mat # reclassify original raster using the jenks classifications nvdi_jenks_recl &lt;- reclassify(nvdi, nvdi_jenks_cl_mat) # 5. arid # Extract values from Raster aridValues &lt;- values(arid) # Change the values from vector object to data.frame object aridDF &lt;- as.data.frame(aridValues) # Remove missing values and reapply column name aridDF &lt;- as.data.frame(aridDF[!is.na(aridDF$aridValues),]) colnames(aridDF) &lt;- &quot;aridValues&quot; # Use the getJenksBreaks() function. Sample 0.10 (10%) of the pixels at random and base the categorisation on this. # NOTE: Doing this on the full data will take forever - so use the subset argument. # EXTRA NOTE: The values for aridity are very close to each other and so the algorithm splits it to just two cateogries aridJenks &lt;- getJenksBreaks(aridDF$aridValues, 2, subset = nrow(aridDF)*0.10) # See value in vector aridJenks # shows first element aridJenks[1] # shows second element aridJenks[2] # so on and so further... # Create categorisation by using the Jenks values in the vector arid_jenks_cl &lt;- c(arid@data@min-1, aridJenks[1], 1, aridJenks[1], arid@data@max+1, 2) # create matrix arid_jenks_cl_mat &lt;- matrix(arid_jenks_cl, ncol = 3, byrow = TRUE) # view categorisation in matrix arid_jenks_cl_mat # reclassify original raster using the jenks classifications arid_jenks_recl &lt;- reclassify(arid, arid_jenks_cl_mat) For elevation, the risk of LF.decreases with higher values for elevation. Therefore, after applying the Jenks intervals, we need to flip the raster values accordingly. # 6. elev # Extract values from Raster elevValues &lt;- values(elev) # Change the values from vector object to data.frame object elevDF &lt;- as.data.frame(elevValues) # Remove missing values and reapply column name elevDF &lt;- as.data.frame(elevDF[!is.na(elevDF$elevValues),]) colnames(elevDF) &lt;- &quot;elevValues&quot; # Use the getJenksBreaks() function. Sample 0.10 (10%) of the pixels at random and base the categorisation on this. # NOTE: Doing this on the full data will take forever - so use the subset argument. elevJenks &lt;- getJenksBreaks(elevDF$elevValues, 10, subset = nrow(elevDF)*0.10) # See value in vector elevJenks # shows first element elevJenks[1] # shows second element elevJenks[2] # so on and so further... # Create categorisation by using the Jenks values in the vector elev_jenks_cl &lt;- c(elev@data@min-1, elevJenks[1], 1, elevJenks[1], elevJenks[2], 2, elevJenks[2], elevJenks[3], 3, elevJenks[3], elevJenks[4], 4, elevJenks[4], elevJenks[5], 5, elevJenks[5], elevJenks[6], 6, elevJenks[6], elevJenks[7], 7, elevJenks[7], elevJenks[8], 8, elevJenks[8], elevJenks[9], 9, elevJenks[9], elev@data@max+1, 10) # create matrix elev_jenks_cl_mat &lt;- matrix(elev_jenks_cl, ncol = 3, byrow = TRUE) # view categorisation in matrix elev_jenks_cl_mat # reclassify original raster using the jenks classifications elev_jenks_recl &lt;- reclassify(elev, elev_jenks_cl_mat) # Now flip the values using raster.invert() function rev_elev_jenks_recl &lt;- raster.invert(elev_jenks_recl) Visualize the inverted output (NOTE: Blue: High elevation, Red: low elevation): tm_shape(rev_elev_jenks_recl) + tm_raster(style = &quot;cont&quot;, title = &quot;Inverted Elev (on Jenks scale)&quot;, palette= &quot;-Spectral&quot;) + tm_shape(kenya_states) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) 4.3.2 Determining the weight of each variable using Saaty’s AHP To estimate the weights, a helpful tool has been developed and provided in an Excel Worksheet. It has been created for you so that you can see how the calculations are carryout step-by-step to derive the weights. You can interact with the cells to see the formulas and explanations are provided at each step of the way. Open the spreadsheet Guidance - AHP Calculator Tool.xlsx and read &amp; follow through the steps carefully. These are steps summarized here: Pairwise comparison &amp; Judgement values: Carry out a pairwise comparison of the factors by constructing a matrix as seen in step 1 in the spreadsheet. Use the criteria and make a “Judgement” on the pairwise comparison (it could be based on expert opinion or from literature) asserting a variable’s importance over the other. Matrices: Create a square matrix with the judgement values inserted accordingly (see step 2) Reciprocals: Take the reciprocals in the matrix (see step 3) Take column sums: Sum each of the columns accordingly (see step 4) Normalization of matrix: For each element in a column in that square matrix, divide it by it’s corresponding column sum. Repeat this step for all other elements in that matrix. Once completed, sum-up the elements from the division for each row and finally divide it by the number of variables to obtain the priority vector or weights. You know that the calculations are correct if the sum of the weights are equal to 1 (see step 5). Validation of whether judgement values are reasonable: We need to calculate a Consistency Ratio (CR), which is derived from the Consistency Index (CI) divided by the Random Index (RI). For the CI, we estimate an eigenvalue which is derived from the summed products between the summed column multplied by the weights (see step 6 and click on the cell C75 to view formula). Use the eigenvalue and estimate the CI (see formula in the notes for step 6). Next, use the Random Index table (developed by Saaty, 1980) to determine the RI based on the dimension of the matrix (in this case, it is 6). Finally, calculate the CR by dividing the CI/RI (see step 6 and click on the cell E91 to view the formula). If CI &lt; 1., the judgement values assigned in step 1 are fine. If CR is bigger than 1.0, then judgement values in pairwise comparison in step 1 were unreasonable (and thus you’ll have to repeat the step with different values again!) 4.3.3 Weighted Linear Combination (WLC) and derivation of Suitability Maps based from AHP analysis Our model uses the Weighted Linear Combination (WLC) approach as the decision rule. The formula to estimate the suitability of LF is as follows: Suitability (LF) = \\(w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + w_5x_5 + w_6x_6\\) The \\(w_i\\) and \\(x_i\\) represent the weights deprived from AHP analysis and raster variables. The weights are multiplied to it corresponding raster to obtain a raster with values weighted but ultimately scaled with an upper limit of 10 (but slightly adjusted for NDVI and Aridity because their scaled to 2). This is the derived formula to use in calculating the suitability regions for LF in RStudio: # use the rescaled columns in the formula (not the originals!) suitablemap_WLC &lt;- 0.3324*prec_jenks_recl + 0.2775*temp_jenks_recl + 0.1571*popl_jenks_recl + 0.0901*nvdi_jenks_recl + 0.0767*arid_jenks_recl + 0.0659*rev_elev_jenks_recl suitablemap_WLC Finally, visualize the output: tm_shape(suitablemap_WLC) + tm_raster(style = &quot;cont&quot;, title = &quot;LF Suitability (AHP WLC)&quot;, palette= &quot;-Spectral&quot;) + tm_shape(kenya_states) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_text(&quot;NAME_1&quot;, size = &quot;AREA&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE, legend.title.size = 0.5, legend.text.size = 0.5) + tm_scale_bar(position = c(&quot;left&quot;,&quot;bottom&quot;)) + tm_compass(position = c(&quot;right&quot;, &quot;top&quot;)) IMPORTANT NOTES: The suitability ranges are estimated to be from 3.27 to 7.49 (weighted on a scale with a upper limit of 10). The highest suitability for LF are regions with values closest to 7.49 and vice versa. 4.4 References (see reading list) 4.4.1 Recommended reading Paper: R.W. Saaty (1987). The Analytical Hierarchy Process - What it is and how it is used, Mathl Modelling, Volume 9(3-5), pages 161 to 176 LINK Technical Document: IDRISI Kilimanjaro: Guide to GIS and Image Processing. Chapter 12: Decision Support - Decision Strategy Analysis, Section on ‘A Typology of Decisions’, pages 151 to 154. DOWNLOAD Paper: A.A. Ali, A. Ahmad (2019). Spatial susceptibility analysis of vector-borne diseases in KMC using geospatial technique and MCDM approach, Modeling Earth Systems and Environment, Issue 5, pages 1135–1159 LINK Paper: X. Zhao et al., (2020). Malaria Risk Map Using Spatial Multi-Criteria Decision Analysis along Yunnan Border During the Pre-elimination Period, Am J Trop Med Hyg, Volume 103 (2), pages 793–809 LINK Paper: B. Ahmed (2015). Landslide susceptibility mapping using multi-criteria evaluation techniques in Chittagong Metropolitan Area, Bangladesh. Landslides, Issue 12, pages 1077–1095 LINK 4.4.2 Data Sources The population density 2015 data for Kenya was obtained from Worldpop and resampled at 1km Click Here Raster for annual precipitation was created from bio1_27 and bio1_37, averaged and clipped to Kenya. Obtained for WorldClim Click Here Raster for annual temperature was created from bio1_27 and bio1_37, averaged and clipped to Kenya. Obtained for WorldClim Click Here Aridity Index data was obtained for the Global Aridity and PET Database and clipped to Kenya Click Here Normalized Differenced Vegetation Index was obtained from NASA MODIS Click Here (Registration required). Elevation was obtained from the SRTM CSI CGIAR Project, and cropped to Kenya Click Here "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
